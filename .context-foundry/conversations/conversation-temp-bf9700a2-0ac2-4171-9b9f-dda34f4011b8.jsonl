{"timestamp": "2026-01-16T22:32:09.557894", "event_type": "system", "task_id": "temp-bf9700a2-0ac2-4171-9b9f-dda34f4011b8", "text": "[init]", "is_error": false}
{"timestamp": "2026-01-16T22:36:32.935041", "event_type": "assistant", "task_id": "temp-bf9700a2-0ac2-4171-9b9f-dda34f4011b8", "text": "# Architecture: ETop AM Intelligence System\n\n## Problem Statement\nWe're building a vendor-neutral Account Manager intelligence platform for MSPs that automates quarterly business reviews (QBRs), lifecycle planning, security posture reporting, and outcome planning. The system must pull data from multiple vendor platforms (ConnectWise Manage, Immy.Bot, Microsoft 365), normalize it into a unified model, calculate three composite health scores (Standards Compliance, Risk, Experience), and generate AI-powered narrative QBR PDFs with strict evidence-based guardrails. \n\nThe architecture must support the Sea-Level Ops methodology where AMs focus on business outcomes, not technical details, and must be extensible via an adapter pattern so future PSA/RMM integrations don't require core engine changes.\n\n## Constraints\n- **Adapter pattern is non-negotiable**: New integrations must plug in without modifying core engine code\n- **Human approval gate**: QBRs must be draft status, never auto-published to clients\n- **No hallucinations**: OpenAI must cite evidence (ticket IDs, device lists, policy states) from normalized data\n- **Normalized relational model**: PostgreSQL with proper indexing for multi-vendor data correlation\n- **Phase 1 scope**: ConnectWise Manage, Immy.Bot, M365 only (no traditional RMM, no multi-PSA)\n- **Node.js runtime**: Express API, PostgreSQL database\n- **Three composite scores**: Each 0-100, weighted aggregates with documented formulas\n\n## Options Considered\n\n### Option A: Monolithic Express App with Adapter Layer\n\n**Description:** Single Node.js/Express application with:\n- Adapter modules that implement a common interface (`sync(credentials) \u2192 normalized data`)\n- PostgreSQL database with normalized entity schema\n- Three scoring engine modules that read from normalized tables\n- OpenAI integration layer with strict prompt templates\n- PDFKit-based generator\n\n**Pros:**\n- Simple deployment (single process)\n- Direct database access eliminates network latency between components\n- Easier to reason about data flow and debug\n- Lower operational complexity (no service mesh, no inter-service auth)\n- Adapter pattern isolates vendor-specific logic without microservice overhead\n\n**Cons:**\n- Single point of failure (entire system down if Express crashes)\n- Harder to scale individual components (can't scale PDF generation separately from API)\n- Mixing sync jobs with API requests in same process could cause resource contention\n\n**Complexity:** Medium  \n**Risk:** Low (proven pattern for LOB apps)\n\n---\n\n### Option B: Event-Driven Microservices with Message Queue\n\n**Description:** Separate services for:\n- API Gateway (Express)\n- Sync Worker Pool (consumes jobs from queue)\n- Scoring Engine Service\n- Narrative Generator Service (OpenAI wrapper)\n- PDF Generator Service\n- Shared PostgreSQL database\n- RabbitMQ or Redis Streams for job orchestration\n\n**Pros:**\n- Independent scaling (can add PDF workers during high load)\n- Fault isolation (sync failure doesn't crash API)\n- Natural async processing (QBR generation is inherently async)\n- Better observability boundaries (per-service metrics)\n\n**Cons:**\n- Operational complexity: need to run/monitor 5+ services + message broker\n- Network latency between services\n- Distributed transaction complexity (sync \u2192 normalize \u2192 score spans multiple services)\n- Debugging harder (trace requests across service boundaries)\n- Over-engineered for Phase 1 scope (3 clients? 10 clients?)\n\n**Complexity:** High  \n**Risk:** Medium (introduces infrastructure dependencies before proving product-market fit)\n\n---\n\n### Option C: Serverless Functions (AWS Lambda / Vercel Functions)\n\n**Description:**\n- API routes as serverless functions\n- Adapter sync jobs as long-running Lambda functions\n- PostgreSQL on RDS/Neon\n- OpenAI + PDFKit in Lambda\n- Step Functions for QBR generation orchestration\n\n**Pros:**\n- Auto-scaling per function\n- Pay-per-execution pricing\n- No server management\n\n**Cons:**\n- Cold start latency (PDF generation could timeout)\n- Lambda execution limits (15min max, PDF generation with large datasets risky)\n- Vendor lock-in (hard to migrate off AWS Step Functions)\n- Local development harder (need SAM/Serverless Framework)\n- PostgreSQL connection pooling complexity (Lambda spins up many connections)\n- Immy.Bot/CW Manage sync could exceed execution limits\n\n**Complexity:** High  \n**Risk:** High (execution time limits are a blocker for sync jobs and PDF generation)\n\n---\n\n## Recommended Approach\n\n**Choice:** Option A \u2014 Monolithic Express App with Adapter Layer\n\n**Rationale:**\nThis is a **Phase 1 proof-of-concept** for a vendor-neutral MSP intelligence platform. The primary risk is validating the adapter pattern and scoring algorithm quality, not scale. Option A delivers:\n\n1. **Fastest path to validating the core hypothesis**: Does the adapter pattern work? Do the scores correlate with real business outcomes? Is OpenAI output trustworthy?\n2. **Acceptable technical risk**: A well-architected Express monolith can handle 100+ MSP clients with proper indexing and caching\n3. **Defer scaling complexity**: If Phase 1 succeeds, we can extract bottleneck components (PDF generation, sync workers) into services later\u2014but we ship faster now\n4. **Operational simplicity**: One Docker container, one database, standard Node.js monitoring tools\n\n**Tradeoffs we're accepting:**\n- Can't independently scale PDF generation (mitigated by making it async with job queue table)\n- Single process failure takes down API (mitigated by process manager like PM2 and container restart policies)\n\n**What we're NOT accepting:**\n- We maintain strict adapter boundaries (adapters have zero knowledge of each other)\n- Scoring engine modules are pure functions (easy to extract later)\n- Database schema is normalized and vendor-agnostic (no CW-specific columns in core tables)\n\n---\n\n## Decision Log\n\n| Decision | Choice | Alternatives Rejected | Rationale |\n|----------|--------|----------------------|-----------|\n| Architecture Pattern | Monolithic Express app | Microservices, Serverless | Simplicity for Phase 1; can extract services later if needed |\n| Database | PostgreSQL (relational) | MongoDB, DynamoDB | Normalized vendor-agnostic schema requires relational integrity; complex joins for scoring |\n| PDF Generation | PDFKit (in-process) | External service, headless Chrome | Lightweight, declarative API; no browser overhead |\n| AI Provider | OpenAI GPT-4 | Claude, self-hosted LLM | Best-in-class narrative quality; JSON mode for structured output |\n| Async Jobs | Database-backed queue (pg-boss) | Redis Queue, RabbitMQ | Reuses PostgreSQL; no extra infrastructure; ACID guarantees |\n| ORM | Knex.js (query builder) | Prisma, TypeORM | Lightweight; direct SQL control; easier to optimize complex scoring queries |\n| API Auth | API key per integration | OAuth, JWT | Simpler for server-to-server; can add OAuth for future portal |\n\n---\n\n## Technology Stack\n- **Language:** Node.js 20 LTS (JavaScript)\n- **Framework:** Express 4.x\n- **Database:** PostgreSQL 15+\n- **Query Builder:** Knex.js (with migrations)\n- **Job Queue:** pg-boss (PostgreSQL-backed)\n- **AI/LLM:** OpenAI API (GPT-4)\n- **PDF Generation:** PDFKit\n- **Testing:** Jest + Supertest (API tests)\n- **Validation:** Joi (request/config validation)\n- **Logging:** Winston (structured JSON logs)\n\n---\n\n## System Architecture\n\n**Pattern:** Layered monolith with adapter-based integration layer\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        API Layer (Express)                           \u2502\n\u2502  /sync/connectwise  /sync/immy  /sync/m365                          \u2502\n\u2502  /clients/:id/scores  /clients/:id/qbr/generate                     \u2502\n\u2502  [Handles auth, validation, response formatting]                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Adapter Layer                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502  ConnectWise    \u2502  \u2502    Immy.Bot     \u2502  \u2502      M365       \u2502     \u2502\n\u2502  \u2502    Adapter      \u2502  \u2502     Adapter     \u2502  \u2502     Adapter     \u2502     \u2502\n\u2502  \u2502 sync(creds) \u2192   \u2502  \u2502  sync(creds) \u2192  \u2502  \u2502  sync(creds) \u2192  \u2502     \u2502\n\u2502  \u2502 normalized[]    \u2502  \u2502  normalized[]   \u2502  \u2502  normalized[]   \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502  [Each adapter: fetch vendor data \u2192 transform \u2192 return normalized]  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               Normalized Data Layer (PostgreSQL + Knex)              \u2502\n\u2502  clients \u2502 sites \u2502 contacts \u2502 users \u2502 devices \u2502 agreements \u2502        \u2502\n\u2502  tickets \u2502 controls \u2502 risks \u2502 recommendations                       \u2502\n\u2502  [Relational schema, indexed for scoring queries]                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Insights Engine (Pure Functions)                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Standards Score  \u2502  \u2502   Risk Score     \u2502  \u2502 Experience Score \u2502  \u2502\n\u2502  \u2502  calculateStd()  \u2502  \u2502  calculateRisk() \u2502  \u2502  calculateExp()  \u2502  \u2502\n\u2502  \u2502  \u2192 0-100 + breakdown \u2502  \u2192 0-100 + breakdown \u2502 \u2192 0-100 + breakdown \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502  [Reads normalized data, applies weighted formulas, returns scores] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Output Layer                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502  \u2502  Narrative Generator \u2502      \u2502   PDF Generator      \u2502             \u2502\n\u2502  \u2502  (OpenAI GPT-4)      \u2502  \u2192   \u2502   (PDFKit)           \u2502             \u2502\n\u2502  \u2502  - Trend spotting    \u2502      \u2502   9 sections:        \u2502             \u2502\n\u2502  \u2502  - Executive summary \u2502      \u2502   1. Executive       \u2502             \u2502\n\u2502  \u2502  - Action synthesis  \u2502      \u2502   2. Score Dashboard \u2502             \u2502\n\u2502  \u2502  - QBR prep notes    \u2502      \u2502   3. Top 5 Risks     \u2502             \u2502\n\u2502  \u2502  [Strict prompts,    \u2502      \u2502   4. Top 5 Wins      \u2502             \u2502\n\u2502  \u2502   citation required] \u2502      \u2502   5. Service Story   \u2502             \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502   6. Security Story  \u2502             \u2502\n\u2502                                \u2502   7. Lifecycle       \u2502             \u2502\n\u2502                                \u2502   8. Roadmap         \u2502             \u2502\n\u2502                                \u2502   9. Outcome Plan    \u2502             \u2502\n\u2502                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Job Queue (pg-boss)                             \u2502\n\u2502  qbr_generation_jobs (client_id, status, result_url, created_at)    \u2502\n\u2502  [Async QBR generation, retry logic, human approval workflow]       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## File Structure\n\n```\netop-am-system-1282/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 adapters/\n\u2502   \u2502   \u251c\u2500\u2500 base-adapter.js           # Abstract interface all adapters implement\n\u2502   \u2502   \u251c\u2500\u2500 connectwise-adapter.js    # CW Manage: agreements, tickets, clients, configs\n\u2502   \u2502   \u251c\u2500\u2500 immy-adapter.js           # Immy.Bot: baselines, drift, compliance\n\u2502   \u2502   \u2514\u2500\u2500 m365-adapter.js           # M365: Entra ID, Defender, Secure Score\n\u2502   \u251c\u2500\u2500 engine/\n\u2502   \u2502   \u251c\u2500\u2500 standards-score.js        # Pure function: data \u2192 0-100 + breakdown\n\u2502   \u2502   \u251c\u2500\u2500 risk-score.js             # Pure function: data \u2192 0-100 + breakdown\n\u2502   \u2502   \u2514\u2500\u2500 experience-score.js       # Pure function: data \u2192 0-100 + breakdown\n\u2502   \u251c\u2500\u2500 qbr/\n\u2502   \u2502   \u251c\u2500\u2500 narrative-generator.js    # OpenAI integration with strict prompts\n\u2502   \u2502   \u2514\u2500\u2500 pdf-generator.js          # PDFKit: scores + narratives \u2192 9-section PDF\n\u2502   \u251c\u2500\u2500 routes/\n\u2502   \u2502   \u251c\u2500\u2500 sync.js                   # POST /sync/connectwise, /sync/immy, /sync/m365\n\u2502   \u2502   \u251c\u2500\u2500 clients.js                # GET /clients/:id/scores\n\u2502   \u2502   \u2514\u2500\u2500 qbr.js                    # POST /clients/:id/qbr/generate, GET /clients/:id/qbr/:jobId\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 client.js                 # Knex query helpers for clients table\n\u2502   \u2502   \u251c\u2500\u2500 device.js                 # Knex query helpers for devices table\n\u2502   \u2502   \u251c\u2500\u2500 ticket.js                 # Knex query helpers for tickets table\n\u2502   \u2502   \u2514\u2500\u2500 ...                       # (one per entity type)\n\u2502   \u251c\u2500\u2500 db/\n\u2502   \u2502   \u251c\u2500\u2500 knexfile.js               # Knex configuration\n\u2502   \u2502   \u2514\u2500\u2500 migrations/               # Timestamped migration files\n\u2502   \u2502       \u251c\u2500\u2500 001_create_clients.js\n\u2502   \u2502       \u251c\u2500\u2500 002_create_sites.js\n\u2502   \u2502       \u251c\u2500\u2500 003_create_devices.js\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 middleware/\n\u2502   \u2502   \u251c\u2500\u2500 auth.js                   # API key validation\n\u2502   \u2502   \u251c\u2500\u2500 error-handler.js          # Centralized error responses\n\u2502   \u2502   \u2514\u2500\u2500 validation.js             # Joi schema validation\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 logger.js                 # Winston configuration\n\u2502   \u2502   \u2514\u2500\u2500 cache.js                  # Simple in-memory cache for scores\n\u2502   \u251c\u2500\u2500 config.js                     # Centralized config (reads from .env)\n\u2502   \u251c\u2500\u2500 app.js                        # Express app setup (routes, middleware)\n\u2502   \u251c\u2500\u2500 server.js                     # HTTP server + graceful shutdown\n\u2502   \u2514\u2500\u2500 worker.js                     # pg-boss worker (processes QBR jobs)\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 unit/\n\u2502   \u2502   \u251c\u2500\u2500 engine/                   # Test scoring formulas with known inputs\n\u2502   \u2502   \u2514\u2500\u2500 adapters/                 # Test adapter normalization logic\n\u2502   \u251c\u2500\u2500 integration/\n\u2502   \u2502   \u2514\u2500\u2500 api/                      # Test API endpoints with test DB\n\u2502   \u251c\u2500\u2500 e2e/\n\u2502   \u2502   \u2514\u2500\u2500 qbr-generation.test.js    # Full flow: mock data \u2192 PDF output\n\u2502   \u2514\u2500\u2500 mocks/\n\u2502       \u251c\u2500\u2500 connectwise-response.json # Sample CW Manage API response\n\u2502       \u251c\u2500\u2500 immy-response.json        # Sample Immy.Bot API response\n\u2502       \u2514\u2500\u2500 m365-response.json        # Sample Microsoft Graph response\n\u251c\u2500\u2500 .env.example                      # Template for required env vars\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 package-lock.json\n\u251c\u2500\u2500 Dockerfile                        # Production container image\n\u251c\u2500\u2500 docker-compose.yml                # Local dev: app + PostgreSQL\n\u251c\u2500\u2500 jest.config.js                    # Jest configuration\n\u2514\u2500\u2500 README.md                         # Setup guide, API docs, architecture overview\n```\n\n---\n\n## Component Interfaces\n\n### API Layer \u2192 Adapter Layer\n\n**Method:** Direct function call  \n**Contract:**\n```javascript\n// All adapters must implement this interface\ninterface BaseAdapter {\n  /**\n   * Sync data from vendor platform\n   * @param {Object} credentials - Vendor-specific auth credentials\n   * @returns {Promise<NormalizedData>} - Normalized entities\n   * @throws {AdapterError} - If API call fails or data invalid\n   */\n  async sync(credentials): Promise<{\n    clients: Client[],\n    sites: Site[],\n    contacts: Contact[],\n    users: User[],\n    devices: Device[],\n    agreements: Agreement[],\n    tickets: Ticket[],\n    controls: Control[],\n    risks: Risk[],\n    recommendations: Recommendation[]\n  }>\n}\n```\n\n**Errors:**\n- `AdapterError` (401: auth failure, 429: rate limit, 500: vendor API down)\n\n---\n\n### Adapter Layer \u2192 Database Layer\n\n**Method:** Knex.js transaction (batch insert/upsert)  \n**Contract:**\n```javascript\n// After adapter returns normalized data, persist to DB\nasync function persistNormalizedData(normalizedData, trx) {\n  // Upsert clients (keyed by external vendor ID)\n  await trx('clients').insert(normalizedData.clients).onConflict('external_id').merge();\n  \n  // Cascade inserts for related entities\n  await trx('sites').insert(normalizedData.sites).onConflict('external_id').merge();\n  // ... repeat for all entities\n  \n  return { synced_at: new Date(), record_counts: {...} };\n}\n```\n\n**Errors:**\n- `DatabaseError` (constraint violation, connection timeout)\n\n---\n\n### API Layer \u2192 Insights Engine\n\n**Method:** Direct function call (pure function, no side effects)  \n**Contract:**\n```javascript\n/**\n * Calculate Standards Compliance score\n * @param {number} clientId - Client database ID\n * @returns {Promise<ScoreResult>}\n */\nasync function calculateStandardsScore(clientId): Promise<{\n  score: number,              // 0-100\n  breakdown: {\n    device_coverage: { score: number, weight: number, evidence: {...} },\n    immy_compliance: { score: number, weight: number, evidence: {...} },\n    patch_compliance: { score: number, weight: number, evidence: {...} },\n    edr_health: { score: number, weight: number, evidence: {...} },\n    m365_secure_score: { score: number, weight: number, evidence: {...} }\n  },\n  computed_at: Date\n}>\n```\n\n**Errors:**\n- `InsufficientDataError` (not enough data to calculate score)\n\n---\n\n### Insights Engine \u2192 Narrative Generator\n\n**Method:** Direct function call (passes scores + evidence)  \n**Contract:**\n```javascript\n/**\n * Generate AI narrative from scores and evidence\n * @param {Object} input - Scores + normalized data excerpts\n * @returns {Promise<Narrative>}\n */\nasync function generateNarrative(input): Promise<{\n  trends: string,               // \"Ticket volume decreased 15% QoQ...\"\n  executive_summary: string,    // Plain English, business outcomes\n  recommendations: Array<{\n    title: string,\n    description: string,\n    priority: 'high' | 'medium' | 'low',\n    evidence: string[]          // REQUIRED: ticket IDs, device names, etc.\n  }>,\n  discussion_points: string[]   // Topics for QBR meeting\n}>\n```\n\n**Errors:**\n- `OpenAIError` (API failure, token limit exceeded, content filter triggered)\n- `HallucinationDetectedError` (validation finds claims without evidence)\n\n---\n\n### Narrative Generator \u2192 PDF Generator\n\n**Method:** Direct function call  \n**Contract:**\n```javascript\n/**\n * Generate QBR PDF\n * @param {Object} data - Scores + narratives + client metadata\n * @returns {Promise<Buffer>} - PDF binary\n */\nasync function generateQBR(data): Promise<Buffer>\n```\n\n**Errors:**\n- `PDFGenerationError` (template error, data formatting issue)\n\n---\n\n### API Layer \u2192 Job Queue\n\n**Method:** pg-boss publish/subscribe  \n**Contract:**\n```javascript\n// Publish QBR generation job\nawait boss.publish('qbr-generation', {\n  client_id: 123,\n  requested_by: 'user@msp.com',\n  options: { include_lifecycle: true }\n});\n\n// Worker subscribes and processes\nboss.subscribe('qbr-generation', async (job) => {\n  const { client_id, options } = job.data;\n  \n  // Execute: scores \u2192 narrative \u2192 PDF\n  const result = await generateFullQBR(client_id, options);\n  \n  return result; // pg-boss stores in job result field\n});\n```\n\n**Errors:**\n- Job retry (3 attempts with exponential backoff)\n- Dead letter queue after max retries\n\n---\n\n## Module Specifications\n\n### Module: ConnectWise Adapter\n**Responsibility:** Fetch and normalize ConnectWise Manage data  \n**Key Files:** `src/adapters/connectwise-adapter.js`  \n**Dependencies:** axios (HTTP), date-fns (date parsing)  \n**Public Interface:**\n```javascript\nclass ConnectWiseAdapter extends BaseAdapter {\n  async sync(credentials): Promise<NormalizedData>\n  \n  // Internal methods (not exported)\n  private async fetchClients()\n  private async fetchAgreements()\n  private async fetchTickets()\n  private async fetchConfigurations()\n  private normalizeClient(cwClient): Client\n  private normalizeTicket(cwTicket): Ticket\n}\n```\n\n---\n\n### Module: Immy.Bot Adapter\n**Responsibility:** Fetch and normalize Immy.Bot compliance data  \n**Key Files:** `src/adapters/immy-adapter.js`  \n**Dependencies:** axios  \n**Public Interface:**\n```javascript\nclass ImmyAdapter extends BaseAdapter {\n  async sync(credentials): Promise<NormalizedData>\n  \n  private async fetchComputers()\n  private async fetchBaselines()\n  private async fetchDriftReports()\n  private normalizeDevice(immyComputer): Device\n  private normalizeControl(immyBaseline): Control\n}\n```\n\n---\n\n### Module: M365 Adapter\n**Responsibility:** Fetch and normalize Microsoft 365 security data  \n**Key Files:** `src/adapters/m365-adapter.js`  \n**Dependencies:** @microsoft/microsoft-graph-client  \n**Public Interface:**\n```javascript\nclass M365Adapter extends BaseAdapter {\n  async sync(credentials): Promise<NormalizedData>\n  \n  private async fetchUsers()           // Entra ID users\n  private async fetchSecureScore()     // Microsoft Secure Score\n  private async fetchDefenderAlerts()  // Defender for Endpoint\n  private normalizeUser(graphUser): User\n  private normalizeRisk(defenderAlert): Risk\n  private normalizeControl(secureScoreControl): Control\n}\n```\n\n---\n\n### Module: Standards Score Engine\n**Responsibility:** Calculate Standards Compliance (0-100)  \n**Key Files:** `src/engine/standards-score.js`  \n**Dependencies:** Knex (database queries)  \n**Public Interface:**\n```javascript\nexport async function calculateStandardsScore(clientId): Promise<ScoreResult>\n\n// Weighted components (internal)\n- deviceCoverageScore()      // 20%: % devices under management\n- immyComplianceScore()      // 30%: % devices passing Immy baselines\n- patchComplianceScore()     // 20%: % devices patched (30-day window)\n- edrHealthScore()           // 15%: EDR installed + reporting\n- m365SecureScore()          // 15%: Microsoft Secure Score normalized\n```\n\n---\n\n### Module: Risk Score Engine\n**Responsibility:** Calculate Risk score (0-100, inverse: higher = more risk)  \n**Key Files:** `src/engine/risk-score.js`  \n**Dependencies:** Knex  \n**Public Interface:**\n```javascript\nexport async function calculateRiskScore(clientId): Promise<ScoreResult>\n\n// Weighted components\n- identityRiskScore()        // 30%: MFA coverage, risky users, stale accounts\n- emailRiskScore()           // 25%: Defender for Office alerts, phishing simulations\n- endpointRiskScore()        // 25%: Unpatched devices, missing EDR, high-risk detections\n- businessModifierScore()    // 20%: Industry (healthcare +10 risk), compliance requirements\n```\n\n---\n\n### Module: Experience Score Engine\n**Responsibility:** Calculate Experience score (0-100)  \n**Key Files:** `src/engine/experience-score.js`  \n**Dependencies:** Knex  \n**Public Interface:**\n```javascript\nexport async function calculateExperienceScore(clientId): Promise<ScoreResult>\n\n// Weighted components\n- ticketsPerUserTrend()      // 25%: Trending down = higher score\n- repeatIssueRate()          // 20%: Same issue recurring = lower score\n- slaPerformance()           // 25%: % tickets meeting SLA\n- reopenRate()               // 15%: % tickets reopened\n- afterHoursIncidents()      // 15%: Fewer = higher score\n```\n\n---\n\n### Module: Narrative Generator\n**Responsibility:** Generate AI narrative with citations  \n**Key Files:** `src/qbr/narrative-generator.js`  \n**Dependencies:** openai (OpenAI SDK)  \n**Public Interface:**\n```javascript\nexport async function generateNarrative(input: {\n  client: Client,\n  scores: { standards: ScoreResult, risk: ScoreResult, experience: ScoreResult },\n  recent_tickets: Ticket[],\n  top_risks: Risk[],\n  lifecycle_items: Device[]\n}): Promise<Narrative>\n\n// Internal: four OpenAI jobs\n- spotTrends()               // Analyze quarter-over-quarter movement\n- writeExecutiveSummary()    // Plain English business outcomes\n- synthesizeActions()        // Recommendations with effort/cost\n- prepareDiscussionPoints()  // Topics for live QBR meeting\n```\n\n**Prompt Template (Example for Executive Summary):**\n```\nYou are an Account Manager writing a quarterly business review for {client.name}.\n\nSTRICT RULES:\n1. Only use facts from the PROVIDED DATA below. Never invent ticket IDs, device names, or metrics.\n2. Every claim must cite evidence: [Ticket #12345], [Device: WS-ACCT-01], [Policy: MFA Enforcement].\n3. Write in plain English for a non-technical executive audience.\n4. Focus on business outcomes (user productivity, risk reduction, cost avoidance), not technical details.\n\nPROVIDED DATA:\n{JSON.stringify(input, null, 2)}\n\nWrite a 3-paragraph executive summary:\n1. Overall health this quarter (reference the three scores)\n2. Biggest win (with evidence)\n3. Top priority for next quarter (with business justification)\n```\n\n---\n\n### Module: PDF Generator\n**Responsibility:** Render 9-section QBR PDF  \n**Key Files:** `src/qbr/pdf-generator.js`  \n**Dependencies:** pdfkit, pdfkit-table (tables)  \n**Public Interface:**\n```javascript\nexport async function generateQBR(data: {\n  client: Client,\n  scores: ScoreResults,\n  narrative: Narrative,\n  risks: Risk[],\n  wins: string[],\n  lifecycle: Device[],\n  roadmap: Recommendation[]\n}): Promise<Buffer>\n\n// Internal: one function per section\n- renderExecutiveSummary()\n- renderScoreDashboard()      // Gauge charts for 3 scores\n- renderTopRisks()            // Table with business impact\n- renderTopWins()             // Bulleted list\n- renderServiceStory()        // Ticket trend charts\n- renderSecurityStory()       // Identity/Email/Endpoint breakdown\n- renderLifecycleForecast()   // Aging device table + 12-36mo budget\n- renderRoadmap()             // Now/Next/Later columns\n- renderOutcomePlan()         // Action items with owners + dates\n```\n\n---\n\n## Data Models\n\n### PostgreSQL Schema (Normalized)\n\n```sql\n-- Core entities\nCREATE TABLE clients (\n  id SERIAL PRIMARY KEY,\n  external_id VARCHAR(255) UNIQUE NOT NULL, -- Vendor-specific ID\n  source VARCHAR(50) NOT NULL,              -- 'connectwise', 'immy', 'm365'\n  name VARCHAR(255) NOT NULL,\n  segment CHAR(1) CHECK (segment IN ('A', 'B', 'C', 'D')),\n  mrr DECIMAL(10,2),\n  agreement_start DATE,\n  agreement_end DATE,\n  created_at TIMESTAMP DEFAULT NOW(),\n  updated_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_clients_external ON clients(external_id, source);\n\nCREATE TABLE sites (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  name VARCHAR(255),\n  address TEXT,\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_sites_client ON sites(client_id);\n\nCREATE TABLE contacts (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  role VARCHAR(100),                        -- 'CEO', 'CFO', 'IT Manager', etc.\n  email VARCHAR(255),\n  phone VARCHAR(50),\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_contacts_client ON contacts(client_id);\n\nCREATE TABLE users (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  email VARCHAR(255),\n  upn VARCHAR(255),                         -- User Principal Name (M365)\n  mfa_enabled BOOLEAN DEFAULT FALSE,\n  risk_level VARCHAR(20),                   -- 'none', 'low', 'medium', 'high'\n  last_sign_in TIMESTAMP,\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_users_client ON users(client_id);\nCREATE INDEX idx_users_risk ON users(risk_level);\n\nCREATE TABLE devices (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  site_id INTEGER REFERENCES sites(id) ON DELETE SET NULL,\n  external_id VARCHAR(255),\n  name VARCHAR(255),\n  type VARCHAR(50),                         -- 'endpoint', 'server', 'network'\n  os VARCHAR(100),\n  managed BOOLEAN DEFAULT FALSE,\n  health_status VARCHAR(20),                -- 'healthy', 'warning', 'critical'\n  last_seen TIMESTAMP,\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_devices_client ON devices(client_id);\nCREATE INDEX idx_devices_health ON devices(health_status);\n\nCREATE TABLE agreements (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  mrr DECIMAL(10,2),\n  effective_rate DECIMAL(10,2),\n  term_months INTEGER,\n  start_date DATE,\n  end_date DATE,\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE tickets (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  category VARCHAR(100),\n  priority VARCHAR(20),\n  status VARCHAR(50),\n  hours_spent DECIMAL(5,2),\n  sla_met BOOLEAN,\n  reopen_count INTEGER DEFAULT 0,\n  csat_score INTEGER,                       -- 1-5 or NULL\n  created_date TIMESTAMP,\n  closed_date TIMESTAMP,\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_tickets_client ON tickets(client_id);\nCREATE INDEX idx_tickets_created ON tickets(created_date);\n\nCREATE TABLE controls (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  control_type VARCHAR(100),                -- 'immy_baseline', 'm365_secure_score', etc.\n  status VARCHAR(20),                       -- 'pass', 'fail', 'unknown'\n  evidence JSONB,                           -- Vendor-specific proof\n  last_checked TIMESTAMP,\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_controls_client ON controls(client_id);\nCREATE INDEX idx_controls_status ON controls(status);\n\nCREATE TABLE risks (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  risk_type VARCHAR(100),                   -- 'identity', 'email', 'endpoint', 'business'\n  title VARCHAR(255),\n  description TEXT,\n  likelihood VARCHAR(20),                   -- 'low', 'medium', 'high'\n  impact VARCHAR(20),                       -- 'low', 'medium', 'high'\n  status VARCHAR(20),                       -- 'open', 'mitigated', 'accepted'\n  detected_at TIMESTAMP,\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_risks_client ON risks(client_id);\nCREATE INDEX idx_risks_status ON risks(status);\n\nCREATE TABLE recommendations (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  title VARCHAR(255),\n  description TEXT,\n  effort VARCHAR(20),                       -- 'low', 'medium', 'high'\n  cost_range VARCHAR(50),                   -- '$1-5K', '$5-10K', etc.\n  priority VARCHAR(20),                     -- 'high', 'medium', 'low'\n  quarter VARCHAR(10),                      -- 'Q1 2026', 'Q2 2026', etc.\n  status VARCHAR(20) DEFAULT 'pending',     -- 'pending', 'approved', 'completed'\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_recommendations_client ON recommendations(client_id);\n\n-- Job queue (pg-boss auto-creates tables, but documenting schema)\n-- pg-boss manages: job, archive, version, schedule tables\n```\n\n---\n\n## Implementation Steps\n\n1. **Project Setup**\n   - Initialize Node.js project (`npm init`)\n   - Install dependencies (Express, Knex, PostgreSQL driver, PDFKit, OpenAI, pg-boss)\n   - Create `.env.example` with required environment variables\n   - Set up ESLint + Prettier\n\n2. **Database Layer**\n   - Configure Knex (`knexfile.js`)\n   - Write migrations for all 10 entity tables\n   - Create model files (query helpers for each table)\n   - Seed test database with mock data\n\n3. **Adapter Layer (Critical Path)**\n   - Implement `BaseAdapter` abstract class\n   - Build ConnectWise adapter (clients, agreements, tickets, devices)\n   - Build Immy adapter (devices, controls)\n   - Build M365 adapter (users, risks, controls)\n   - Write unit tests with mocked API responses\n\n4. **API Routes (Sync)**\n   - `POST /sync/connectwise` \u2192 calls adapter \u2192 persists to DB\n   - `POST /sync/immy` \u2192 calls adapter \u2192 persists to DB\n   - `POST /sync/m365` \u2192 calls adapter \u2192 persists to DB\n   - Add API key auth middleware\n   - Add request validation (Joi schemas)\n\n5. **Scoring Engine**\n   - Implement `calculateStandardsScore()` with weighted components\n   - Implement `calculateRiskScore()` with weighted components\n   - Implement `calculateExperienceScore()` with weighted components\n   - Write unit tests with known input/output pairs\n   - Create `GET /clients/:id/scores` endpoint\n\n6. **Narrative Generator**\n   - Create OpenAI prompt templates (4 jobs: trends, summary, actions, prep)\n   - Implement citation validation (regex check for ticket IDs, device names)\n   - Add retry logic for OpenAI API failures\n   - Write tests with mocked OpenAI responses\n\n7. **PDF Generator**\n   - Scaffold 9-section PDF structure with PDFKit\n   - Implement gauge charts for score dashboard\n   - Implement tables for risks, lifecycle, roadmap\n   - Test with various data volumes (edge case: client with 500 devices)\n\n8. **Job Queue (Async QBR Generation)**\n   - Set up pg-boss\n   - Create `qbr-generation` job handler (scores \u2192 narrative \u2192 PDF)\n   - Implement `POST /clients/:id/qbr/generate` (publishes job, returns job ID)\n   - Implement `GET /clients/:id/qbr/:jobId` (checks job status, returns PDF URL)\n   - Add retry logic (3 attempts)\n\n9. **End-to-End Testing**\n   - Seed test DB with mock data from all 3 adapters\n   - Trigger full QBR generation flow\n   - Validate PDF output contains all 9 sections with real data\n   - Verify OpenAI citations reference actual ticket IDs\n\n10. **Deployment Prep**\n    - Create Dockerfile (multi-stage build)\n    - Create `docker-compose.yml` (app + PostgreSQL)\n    - Document environment variables in README\n    - Add health check endpoint (`GET /health`)\n\n---\n\n## Testing Strategy\n\n### Unit Tests\n- **Adapters**: Test normalization logic with mocked vendor API responses\n  - Input: `connectwise-response.json` \u2192 Output: `Client[]` with correct fields\n  - Validate error handling (401, 429, 500 from vendor API)\n- **Scoring Engine**: Test formulas with known inputs\n  - Example: 80% devices managed, 90% Immy compliance \u2192 Standards score = 85\n  - Test edge cases: zero devices, missing data\n- **Narrative Generator**: Test prompt construction and citation extraction\n  - Mock OpenAI response \u2192 validate citations match provided ticket IDs\n  - Test hallucination detection (claim without evidence \u2192 error)\n\n### Integration Tests\n- **API Endpoints**: Test routes with test database\n  - `POST /sync/connectwise` \u2192 verify DB tables populated\n  - `GET /clients/:id/scores` \u2192 verify scores calculated from DB data\n  - `POST /clients/:id/qbr/generate` \u2192 verify job created in queue\n- **Database**: Test migrations run cleanly (up/down)\n- **Job Queue**: Test pg-boss job processing with mocked QBR generation\n\n### End-to-End Tests\n- **Full QBR Flow**:\n  1. Seed DB with mock data from `tests/mocks/`\n  2. Trigger `POST /clients/1/qbr/generate`\n  3. Wait for job completion\n  4. Download PDF\n  5. Validate PDF contains:\n     - All 9 sections\n     - Real data (ticket IDs, device names from mocked data)\n     - OpenAI narratives with citations\n     - Charts render correctly\n\n### Performance Tests (Future)\n- Sync 1000 tickets from ConnectWise \u2192 measure DB insert time\n- Generate QBR for client with 500 devices \u2192 measure total time (target: <60s)\n\n---\n\n## Success Criteria\n\nPhase 1 is **DONE** when:\n\n1. \u2705 **Adapter Pattern Proven**\n   - All 3 adapters sync mock data \u2192 normalized DB tables populated\n   - Adding a 4th adapter requires zero changes to engine/qbr modules\n\n2. \u2705 **Scores Calculate Correctly**\n   - Standards, Risk, Experience scores return 0-100 values\n   - Breakdown explains weighted components with evidence\n   - Edge case: insufficient data \u2192 returns error, not invalid score\n\n3. \u2705 **OpenAI Generates Trustworthy Narratives**\n   - Every recommendation cites evidence (ticket IDs, device names)\n   - Hallucination validation catches claims without evidence\n   - Narratives are plain English suitable for non-technical executives\n\n4. \u2705 **PDF Contains All 9 Sections with Real Data**\n   - Executive Summary (narratives)\n   - Score Dashboard (3 gauges with movement explanation)\n   - Top 5 Risks (table with business impact)\n   - Top 5 Wins (list)\n   - Service Experience Story (ticket trends)\n   - Security Posture Story (identity/email/endpoint)\n   - Lifecycle Forecast (aging devices + budget)\n   - Roadmap (Now/Next/Later)\n   - Outcome Plan (actions with owners/dates)\n\n5. \u2705 **End-to-End Test Passes**\n   - Mock data \u2192 sync \u2192 score \u2192 QBR PDF generation succeeds\n   - PDF visually reviewed for correctness\n   - No errors in logs\n\n6. \u2705 **Documentation Complete**\n   - README explains setup, API usage, environment variables\n   - Architecture decision log documented\n   - Code comments explain scoring formulas and weights\n\n---\n\n## Acceptance Criteria (Gherkin)\n\n### Feature: ConnectWise Adapter Sync\n\nSyncs clients, agreements, tickets, and devices from ConnectWise Manage and normalizes them into the database.\n\n**Background:**\n```gherkin\nGiven the database is empty\nAnd I have valid ConnectWise Manage credentials in environment variables\nAnd ConnectWise Manage contains:\n  | Entity      | Count |\n  | Clients     | 5     |\n  | Agreements  | 8     |\n  | Tickets     | 150   |\n  | Devices     | 47    |\n```\n\n**Scenario: Successfully sync all ConnectWise data**\n```gherkin\nWhen I POST to \"/sync/connectwise\" with valid credentials\nThen the response status should be 200\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"synced_at\": \"<timestamp>\",\n    \"record_counts\": {\n      \"clients\": 5,\n      \"agreements\": 8,\n      \"tickets\": 150,\n      \"devices\": 47\n    }\n  }\n  \"\"\"\nAnd the \"clients\" table should contain 5 records\nAnd the \"agreements\" table should contain 8 records\nAnd the \"tickets\" table should contain 150 records\nAnd the \"devices\" table should contain 47 records\nAnd each client record should have a valid \"external_id\" from ConnectWise\nAnd each client record should have \"source\" = \"connectwise\"\n```\n\n**Scenario: Handle ConnectWise API authentication failure**\n```gherkin\nGiven I have invalid ConnectWise credentials\nWhen I POST to \"/sync/connectwise\" with invalid credentials\nThen the response status should be 401\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"error\": \"ConnectWise authentication failed\",\n    \"details\": \"Invalid API credentials\"\n  }\n  \"\"\"\nAnd no records should be inserted into the database\n```\n\n**Scenario: Handle ConnectWise API rate limit**\n```gherkin\nGiven ConnectWise API returns 429 status (rate limit exceeded)\nWhen I POST to \"/sync/connectwise\"\nThen the adapter should retry with exponential backoff\nAnd after 3 retries, the response status should be 429\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"error\": \"ConnectWise rate limit exceeded\",\n    \"retry_after\": 60\n  }\n  \"\"\"\n```\n\n---\n\n### Feature: Immy.Bot Adapter Sync\n\nSyncs device compliance data from Immy.Bot and normalizes it into devices and controls tables.\n\n**Background:**\n```gherkin\nGiven the database contains 5 clients from ConnectWise sync\nAnd I have valid Immy.Bot credentials\nAnd Immy.Bot contains:\n  | Entity          | Count |\n  | Computers       | 47    |\n  | Baselines       | 3     |\n  | Drift Reports   | 12    |\n```\n\n**Scenario: Successfully sync Immy.Bot compliance data**\n```gherkin\nWhen I POST to \"/sync/immy\" with valid credentials\nThen the response status should be 200\nAnd the \"devices\" table should be updated with 47 records (upsert by external_id)\nAnd the \"controls\" table should contain 12 records with control_type = \"immy_baseline\"\nAnd each control record should have:\n  | Field         | Value                      |\n  | status        | \"pass\", \"fail\", or \"unknown\" |\n  | evidence      | Valid JSON object          |\n  | last_checked  | Timestamp within last 24h  |\n```\n\n**Scenario: Immy.Bot device maps to existing ConnectWise device**\n```gherkin\nGiven a ConnectWise device exists with external_id = \"CW-DEVICE-123\"\nAnd an Immy.Bot computer has the same serial number mapping to \"CW-DEVICE-123\"\nWhen I POST to \"/sync/immy\"\nThen the device record should be updated (not duplicated)\nAnd the device should have data from both ConnectWise and Immy.Bot sources\nAnd the \"managed\" field should be set to true\n```\n\n---\n\n### Feature: Microsoft 365 Adapter Sync\n\nSyncs users, security controls, and risks from Microsoft Graph API.\n\n**Background:**\n```gherkin\nGiven the database contains 5 clients\nAnd I have valid Microsoft 365 credentials (tenant ID, client ID, client secret)\nAnd Microsoft Graph API contains:\n  | Entity               | Count |\n  | Entra ID Users       | 73    |\n  | Secure Score Controls| 28    |\n  | Defender Alerts      | 5     |\n```\n\n**Scenario: Successfully sync Microsoft 365 data**\n```gherkin\nWhen I POST to \"/sync/m365\" with valid credentials\nThen the response status should be 200\nAnd the \"users\" table should contain 73 records\nAnd the \"controls\" table should contain 28 records with control_type = \"m365_secure_score\"\nAnd the \"risks\" table should contain 5 records with risk_type = \"endpoint\"\nAnd each user record should have:\n  | Field        | Constraint                     |\n  | email        | Valid email format             |\n  | upn          | Matches Microsoft UPN format   |\n  | mfa_enabled  | Boolean (true/false)           |\n  | risk_level   | \"none\", \"low\", \"medium\", or \"high\" |\n```\n\n**Scenario: Microsoft Graph API permission error**\n```gherkin\nGiven the Microsoft 365 credentials lack required Graph API scopes\nWhen I POST to \"/sync/m365\"\nThen the response status should be 403\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"error\": \"Microsoft Graph permission denied\",\n    \"required_scopes\": [\"User.Read.All\", \"SecurityEvents.Read.All\", \"SecurityActions.Read.All\"]\n  }\n  \"\"\"\n```\n\n---\n\n### Feature: Calculate Standards Compliance Score\n\nCalculates a 0-100 score based on device coverage, Immy compliance, patch status, EDR health, and M365 Secure Score.\n\n**Background:**\n```gherkin\nGiven client ID 1 has the following data:\n  | Metric                  | Value |\n  | Total devices           | 50    |\n  | Managed devices         | 45    |\n  | Devices passing Immy    | 40    |\n  | Devices patched (30d)   | 42    |\n  | Devices with EDR        | 48    |\n  | M365 Secure Score       | 72%   |\n```\n\n**Scenario: Calculate Standards score with all data present**\n```gherkin\nWhen I GET \"/clients/1/scores\"\nThen the response status should be 200\nAnd the response body should contain a \"standards\" object with:\n  | Field | Value Range |\n  | score | 80-85       |\nAnd the \"standards.breakdown\" should include:\n  | Component         | Weight | Score Range |\n  | device_coverage   | 20%    | 90-100      |\n  | immy_compliance   | 30%    | 80-90       |\n  | patch_compliance  | 20%    | 84-90       |\n  | edr_health        | 15%    | 96-100      |\n  | m365_secure_score | 15%    | 72-72       |\nAnd each breakdown component should have an \"evidence\" object with source data\n```\n\n**Scenario: Insufficient data for Standards score**\n```gherkin\nGiven client ID 2 has zero devices in the database\nWhen I GET \"/clients/2/scores\"\nThen the response status should be 200\nAnd the \"standards\" object should have:\n  \"\"\"\n  {\n    \"score\": null,\n    \"error\": \"Insufficient data: no devices found\",\n    \"breakdown\": {}\n  }\n  \"\"\"\n```\n\n---\n\n### Feature: Calculate Risk Score\n\nCalculates a 0-100 score (inverse: higher = more risk) based on identity, email, endpoint, and business risks.\n\n**Background:**\n```gherkin\nGiven client ID 1 has the following data:\n  | Metric                     | Value |\n  | Total users                | 50    |\n  | Users with MFA enabled     | 45    |\n  | Users with high risk level | 2     |\n  | Defender email alerts      | 3     |\n  | Devices missing EDR        | 2     |\n  | Open critical risks        | 1     |\n```\n\n**Scenario: Calculate Risk score with good security posture**\n```gherkin\nWhen I GET \"/clients/1/scores\"\nThen the response status should be 200\nAnd the \"risk\" object should have a score between 10-20 (low risk)\nAnd the \"risk.breakdown\" should include:\n  | Component           | Weight | Risk Level |\n  | identity_risk       | 30%    | Low        |\n  | email_risk          | 25%    | Low        |\n  | endpoint_risk       | 25%    | Low        |\n  | business_modifier   | 20%    | Medium     |\nAnd the \"risk.breakdown.identity_risk.evidence\" should cite:\n  - \"45/50 users (90%) have MFA enabled\"\n  - \"2 users flagged as high risk by Entra ID\"\n```\n\n**Scenario: Calculate Risk score with poor MFA coverage**\n```gherkin\nGiven client ID 3 has:\n  | Metric                 | Value |\n  | Total users            | 100   |\n  | Users with MFA enabled | 30    |\nWhen I GET \"/clients/3/scores\"\nThen the \"risk.breakdown.identity_risk\" should have a score >= 50 (high risk)\nAnd the evidence should cite \"30/100 users (30%) have MFA enabled\"\n```\n\n---\n\n### Feature: Calculate Experience Score\n\nCalculates a 0-100 score based on ticket volume, repeat issues, SLA performance, reopen rate, and after-hours incidents.\n\n**Background:**\n```gherkin\nGiven client ID 1 has the following ticket data for Q4 2025:\n  | Metric                     | Value |\n  | Total tickets              | 120   |\n  | Total users                | 50    |\n  | Tickets meeting SLA        | 108   |\n  | Tickets reopened           | 5     |\n  | Repeat issues (same cat.)  | 8     |\n  | After-hours tickets        | 12    |\nAnd client ID 1 had 150 tickets in Q3 2025\n```\n\n**Scenario: Calculate Experience score with improving trend**\n```gherkin\nWhen I GET \"/clients/1/scores\"\nThen the response status should be 200\nAnd the \"experience\" object should have a score between 75-85\nAnd the \"experience.breakdown.tickets_per_user_trend\" should cite:\n  - \"Q3 2025: 3.0 tickets/user\"\n  - \"Q4 2025: 2.4 tickets/user\"\n  - \"Improvement: -20%\"\nAnd the \"experience.breakdown.sla_performance\" should have a score >= 85\nAnd the evidence should cite \"108/120 tickets (90%) met SLA\"\n```\n\n**Scenario: High reopen rate lowers Experience score**\n```gherkin\nGiven client ID 2 has 40 tickets with 15 reopens (37.5% reopen rate)\nWhen I GET \"/clients/2/scores\"\nThen the \"experience.breakdown.reopen_rate\" should have a score <= 30\nAnd the evidence should cite \"15/40 tickets (37.5%) were reopened\"\n```\n\n---\n\n### Feature: Generate QBR Narrative with OpenAI\n\nUses OpenAI GPT-4 to generate executive summary, trends, recommendations, and discussion points with mandatory citations.\n\n**Background:**\n```gherkin\nGiven client ID 1 has scores:\n  | Score      | Value |\n  | Standards  | 83    |\n  | Risk       | 15    |\n  | Experience | 78    |\nAnd the database contains:\n  - 120 tickets for Q4 2025\n  - 3 open risks with risk_type = \"identity\"\n  - 5 devices flagged for lifecycle replacement\n```\n\n**Scenario: Generate narrative with valid citations**\n```gherkin\nWhen I POST to \"/clients/1/qbr/generate\"\nThen a job should be created in the queue\nAnd when the job completes, the narrative should include:\n  - \"executive_summary\" (3 paragraphs, plain English, business outcomes)\n  - \"trends\" (quarter-over-quarter analysis)\n  - \"recommendations\" (array of 3-7 items)\n  - \"discussion_points\" (array of 3-5 topics)\nAnd every recommendation should have an \"evidence\" array with at least 1 citation\nAnd citations should match one of these formats:\n  - \"[Ticket #12345]\"\n  - \"[Device: WS-ACCT-01]\"\n  - \"[Policy: MFA Enforcement]\"\n  - \"[Risk: Unpatched Exchange Server]\"\nAnd no recommendation should contain invented data (e.g., ticket IDs not in database)\n```\n\n**Scenario: Detect hallucination in OpenAI output**\n```gherkin\nGiven the OpenAI response contains a recommendation:\n  \"\"\"\n  {\n    \"title\": \"Replace aging firewall\",\n    \"description\": \"Device FW-MAIN-99 is end-of-life\",\n    \"evidence\": [\"[Device: FW-MAIN-99]\"]\n  }\n  \"\"\"\nBut the database does not contain a device with name \"FW-MAIN-99\"\nWhen the narrative validation runs\nThen the validation should fail with error:\n  \"\"\"\n  Hallucination detected: Device \"FW-MAIN-99\" cited in evidence but not found in database\n  \"\"\"\nAnd the job status should be \"failed\"\nAnd a human review flag should be set\n```\n\n**Scenario: OpenAI API timeout**\n```gherkin\nGiven the OpenAI API takes longer than 60 seconds to respond\nWhen the narrative generation job runs\nThen the job should retry up to 3 times with exponential backoff\nAnd if all retries fail, the job status should be \"failed\"\nAnd the error message should be \"OpenAI API timeout after 3 retries\"\n```\n\n---\n\n### Feature: Generate QBR PDF\n\nProduces a 9-section PDF with scores, narratives, risks, wins, lifecycle forecast, and roadmap.\n\n**Background:**\n```gherkin\nGiven client ID 1 has:\n  - Scores: Standards 83, Risk 15, Experience 78\n  - Narrative generated by OpenAI (all sections present)\n  - 5 open risks in database\n  - 8 recommendations for next quarter\n  - 12 devices flagged for lifecycle replacement\n```\n\n**Scenario: Generate complete QBR PDF**\n```gherkin\nWhen the QBR generation job for client ID 1 completes\nThen a PDF file should be created\nAnd the PDF should contain exactly 9 sections in this order:\n  1. Executive Summary\n  2. Score Dashboard (3 gauge charts)\n  3. Top 5 Risks\n  4. Top 5 Wins\n  5. Service Experience Story\n  6. Security Posture Story\n  7. Lifecycle Forecast\n  8. Roadmap (Now/Next/Later)\n  9. Outcome Plan\nAnd the \"Score Dashboard\" section should display:\n  - Standards score: 83 (with visual gauge)\n  - Risk score: 15 (with visual gauge)\n  - Experience score: 78 (with visual gauge)\n  - Movement explanation (e.g., \"Standards +5 vs. Q3\")\nAnd the \"Top 5 Risks\" section should display a table with columns:\n  - Risk Title\n  - Business Impact\n  - Status\n  - Mitigation Action\nAnd the table should contain exactly 5 rows (top risks by likelihood \u00d7 impact)\nAnd the \"Lifecycle Forecast\" section should display:\n  - Table of devices aged 4+ years\n  - 12-month refresh budget estimate\n  - 24-month refresh budget estimate\n  - 36-month refresh budget estimate\n```\n\n**Scenario: PDF renders correctly with large dataset**\n```gherkin\nGiven client ID 2 has 500 devices and 1000 tickets\nWhen the QBR PDF is generated\nThen the PDF should be created without errors\nAnd the file size should be less than 10 MB\nAnd all sections should render completely (no truncation)\n```\n\n**Scenario: Missing data in PDF**\n```gherkin\nGiven client ID 3 has no recommendations in the database\nWhen the QBR PDF is generated\nThen the \"Roadmap\" section should display:\n  \"\"\"\n  No recommendations available for this quarter.\n  \"\"\"\nAnd the PDF should still contain all 9 sections\n```\n\n---\n\n### Feature: Async QBR Generation with Job Queue\n\nQBR generation is asynchronous using pg-boss. Client receives a job ID and can poll for status.\n\n**Background:**\n```gherkin\nGiven client ID 1 exists with all required data\nAnd the pg-boss worker is running\n```\n\n**Scenario: Submit QBR generation job**\n```gherkin\nWhen I POST to \"/clients/1/qbr/generate\"\nThen the response status should be 202 (Accepted)\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"job_id\": \"<uuid>\",\n    \"status\": \"queued\",\n    \"message\": \"QBR generation started. Poll /clients/1/qbr/<job_id> for status.\"\n  }\n  \"\"\"\n```\n\n**Scenario: Poll job status (in progress)**\n```gherkin\nGiven a QBR generation job exists with ID \"abc-123\"\nAnd the job status is \"active\"\nWhen I GET \"/clients/1/qbr/abc-123\"\nThen the response status should be 200\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"job_id\": \"abc-123\",\n    \"status\": \"active\",\n    \"progress\": \"Generating narrative (step 2/4)\"\n  }\n  \"\"\"\n```\n\n**Scenario: Poll job status (completed)**\n```gherkin\nGiven a QBR generation job exists with ID \"abc-123\"\nAnd the job status is \"completed\"\nAnd the PDF file is stored at \"/outputs/qbr-client-1-abc-123.pdf\"\nWhen I GET \"/clients/1/qbr/abc-123\"\nThen the response status should be 200\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"job_id\": \"abc-123\",\n    \"status\": \"completed\",\n    \"pdf_url\": \"/outputs/qbr-client-1-abc-123.pdf\",\n    \"completed_at\": \"<timestamp>\"\n  }\n  \"\"\"\n```\n\n**Scenario: Job fails and retries**\n```gherkin\nGiven the OpenAI API is temporarily down\nWhen a QBR generation job is processed\nThen the job should fail with status \"retry\"\nAnd the job should be retried after 30 seconds (exponential backoff)\nAnd after 3 failed attempts, the job status should be \"failed\"\nAnd the error message should be logged\n```\n\n---\n\n### Feature: Client Segmentation for QBR Cadence\n\nClients are segmented (A/B/C/D) to determine QBR frequency. Segment affects cadence, not content.\n\n**Background:**\n```gherkin\nGiven the database contains clients with segments:\n  | Client ID | Segment | Last QBR Date |\n  | 1         | A       | 2025-10-01    |\n  | 2         | B       | 2025-10-01    |\n  | 3         | C       | 2025-07-01    |\n  | 4         | D       | 2024-10-01    |\nAnd today is 2026-01-15\n```\n\n**Scenario: Segment A/B clients are due for quarterly QBR**\n```gherkin\nWhen I GET \"/clients/due-for-qbr\"\nThen the response should include clients 1 and 2\nAnd the response should explain:\n  - Client 1 (Segment A): Last QBR 2025-10-01, due quarterly, next due 2026-01-01\n  - Client 2 (Segment B): Last QBR 2025-10-01, due quarterly, next due 2026-01-01\n```\n\n**Scenario: Segment C clients are due for semiannual QBR**\n```gherkin\nWhen I GET \"/clients/due-for-qbr\"\nThen the response should include client 3\nAnd the response should explain:\n  - Client 3 (Segment C): Last QBR 2025-07-01, due semiannually, next due 2026-01-01\n```\n\n**Scenario: Segment D clients are due for annual QBR**\n```gherkin\nWhen I GET \"/clients/due-for-qbr\"\nThen the response should include client 4\nAnd the response should explain:\n  - Client 4 (Segment D): Last QBR 2024-10-01, due annually, next due 2025-10-01 (overdue)\n```\n\n**Scenario: All segments receive same QBR quality**\n```gherkin\nGiven I generate QBRs for client 1 (Segment A) and client 4 (Segment D)\nThen both PDFs should contain all 9 sections\nAnd both PDFs should have OpenAI-generated narratives with citations\nAnd both PDFs should have the same level of detail and formatting\n```", "is_error": false}
{"timestamp": "2026-01-16T22:36:32.947467", "event_type": "usage", "task_id": "temp-bf9700a2-0ac2-4171-9b9f-dda34f4011b8", "is_error": false, "input_tokens": 1, "output_tokens": 15459}
{"timestamp": "2026-01-16T22:36:32.947467", "event_type": "result", "task_id": "temp-bf9700a2-0ac2-4171-9b9f-dda34f4011b8", "text": "# Architecture: ETop AM Intelligence System\n\n## Problem Statement\nWe're building a vendor-neutral Account Manager intelligence platform for MSPs that automates quarterly business reviews (QBRs), lifecycle planning, security posture reporting, and outcome planning. The system must pull data from multiple vendor platforms (ConnectWise Manage, Immy.Bot, Microsoft 365), normalize it into a unified model, calculate three composite health scores (Standards Compliance, Risk, Experience), and generate AI-powered narrative QBR PDFs with strict evidence-based guardrails. \n\nThe architecture must support the Sea-Level Ops methodology where AMs focus on business outcomes, not technical details, and must be extensible via an adapter pattern so future PSA/RMM integrations don't require core engine changes.\n\n## Constraints\n- **Adapter pattern is non-negotiable**: New integrations must plug in without modifying core engine code\n- **Human approval gate**: QBRs must be draft status, never auto-published to clients\n- **No hallucinations**: OpenAI must cite evidence (ticket IDs, device lists, policy states) from normalized data\n- **Normalized relational model**: PostgreSQL with proper indexing for multi-vendor data correlation\n- **Phase 1 scope**: ConnectWise Manage, Immy.Bot, M365 only (no traditional RMM, no multi-PSA)\n- **Node.js runtime**: Express API, PostgreSQL database\n- **Three composite scores**: Each 0-100, weighted aggregates with documented formulas\n\n## Options Considered\n\n### Option A: Monolithic Express App with Adapter Layer\n\n**Description:** Single Node.js/Express application with:\n- Adapter modules that implement a common interface (`sync(credentials) \u2192 normalized data`)\n- PostgreSQL database with normalized entity schema\n- Three scoring engine modules that read from normalized tables\n- OpenAI integration layer with strict prompt templates\n- PDFKit-based generator\n\n**Pros:**\n- Simple deployment (single process)\n- Direct database access eliminates network latency between components\n- Easier to reason about data flow and debug\n- Lower operational complexity (no service mesh, no inter-service auth)\n- Adapter pattern isolates vendor-specific logic without microservice overhead\n\n**Cons:**\n- Single point of failure (entire system down if Express crashes)\n- Harder to scale individual components (can't scale PDF generation separately from API)\n- Mixing sync jobs with API requests in same process could cause resource contention\n\n**Complexity:** Medium  \n**Risk:** Low (proven pattern for LOB apps)\n\n---\n\n### Option B: Event-Driven Microservices with Message Queue\n\n**Description:** Separate services for:\n- API Gateway (Express)\n- Sync Worker Pool (consumes jobs from queue)\n- Scoring Engine Service\n- Narrative Generator Service (OpenAI wrapper)\n- PDF Generator Service\n- Shared PostgreSQL database\n- RabbitMQ or Redis Streams for job orchestration\n\n**Pros:**\n- Independent scaling (can add PDF workers during high load)\n- Fault isolation (sync failure doesn't crash API)\n- Natural async processing (QBR generation is inherently async)\n- Better observability boundaries (per-service metrics)\n\n**Cons:**\n- Operational complexity: need to run/monitor 5+ services + message broker\n- Network latency between services\n- Distributed transaction complexity (sync \u2192 normalize \u2192 score spans multiple services)\n- Debugging harder (trace requests across service boundaries)\n- Over-engineered for Phase 1 scope (3 clients? 10 clients?)\n\n**Complexity:** High  \n**Risk:** Medium (introduces infrastructure dependencies before proving product-market fit)\n\n---\n\n### Option C: Serverless Functions (AWS Lambda / Vercel Functions)\n\n**Description:**\n- API routes as serverless functions\n- Adapter sync jobs as long-running Lambda functions\n- PostgreSQL on RDS/Neon\n- OpenAI + PDFKit in Lambda\n- Step Functions for QBR generation orchestration\n\n**Pros:**\n- Auto-scaling per function\n- Pay-per-execution pricing\n- No server management\n\n**Cons:**\n- Cold start latency (PDF generation could timeout)\n- Lambda execution limits (15min max, PDF generation with large datasets risky)\n- Vendor lock-in (hard to migrate off AWS Step Functions)\n- Local development harder (need SAM/Serverless Framework)\n- PostgreSQL connection pooling complexity (Lambda spins up many connections)\n- Immy.Bot/CW Manage sync could exceed execution limits\n\n**Complexity:** High  \n**Risk:** High (execution time limits are a blocker for sync jobs and PDF generation)\n\n---\n\n## Recommended Approach\n\n**Choice:** Option A \u2014 Monolithic Express App with Adapter Layer\n\n**Rationale:**\nThis is a **Phase 1 proof-of-concept** for a vendor-neutral MSP intelligence platform. The primary risk is validating the adapter pattern and scoring algorithm quality, not scale. Option A delivers:\n\n1. **Fastest path to validating the core hypothesis**: Does the adapter pattern work? Do the scores correlate with real business outcomes? Is OpenAI output trustworthy?\n2. **Acceptable technical risk**: A well-architected Express monolith can handle 100+ MSP clients with proper indexing and caching\n3. **Defer scaling complexity**: If Phase 1 succeeds, we can extract bottleneck components (PDF generation, sync workers) into services later\u2014but we ship faster now\n4. **Operational simplicity**: One Docker container, one database, standard Node.js monitoring tools\n\n**Tradeoffs we're accepting:**\n- Can't independently scale PDF generation (mitigated by making it async with job queue table)\n- Single process failure takes down API (mitigated by process manager like PM2 and container restart policies)\n\n**What we're NOT accepting:**\n- We maintain strict adapter boundaries (adapters have zero knowledge of each other)\n- Scoring engine modules are pure functions (easy to extract later)\n- Database schema is normalized and vendor-agnostic (no CW-specific columns in core tables)\n\n---\n\n## Decision Log\n\n| Decision | Choice | Alternatives Rejected | Rationale |\n|----------|--------|----------------------|-----------|\n| Architecture Pattern | Monolithic Express app | Microservices, Serverless | Simplicity for Phase 1; can extract services later if needed |\n| Database | PostgreSQL (relational) | MongoDB, DynamoDB | Normalized vendor-agnostic schema requires relational integrity; complex joins for scoring |\n| PDF Generation | PDFKit (in-process) | External service, headless Chrome | Lightweight, declarative API; no browser overhead |\n| AI Provider | OpenAI GPT-4 | Claude, self-hosted LLM | Best-in-class narrative quality; JSON mode for structured output |\n| Async Jobs | Database-backed queue (pg-boss) | Redis Queue, RabbitMQ | Reuses PostgreSQL; no extra infrastructure; ACID guarantees |\n| ORM | Knex.js (query builder) | Prisma, TypeORM | Lightweight; direct SQL control; easier to optimize complex scoring queries |\n| API Auth | API key per integration | OAuth, JWT | Simpler for server-to-server; can add OAuth for future portal |\n\n---\n\n## Technology Stack\n- **Language:** Node.js 20 LTS (JavaScript)\n- **Framework:** Express 4.x\n- **Database:** PostgreSQL 15+\n- **Query Builder:** Knex.js (with migrations)\n- **Job Queue:** pg-boss (PostgreSQL-backed)\n- **AI/LLM:** OpenAI API (GPT-4)\n- **PDF Generation:** PDFKit\n- **Testing:** Jest + Supertest (API tests)\n- **Validation:** Joi (request/config validation)\n- **Logging:** Winston (structured JSON logs)\n\n---\n\n## System Architecture\n\n**Pattern:** Layered monolith with adapter-based integration layer\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        API Layer (Express)                           \u2502\n\u2502  /sync/connectwise  /sync/immy  /sync/m365                          \u2502\n\u2502  /clients/:id/scores  /clients/:id/qbr/generate                     \u2502\n\u2502  [Handles auth, validation, response formatting]                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Adapter Layer                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502  ConnectWise    \u2502  \u2502    Immy.Bot     \u2502  \u2502      M365       \u2502     \u2502\n\u2502  \u2502    Adapter      \u2502  \u2502     Adapter     \u2502  \u2502     Adapter     \u2502     \u2502\n\u2502  \u2502 sync(creds) \u2192   \u2502  \u2502  sync(creds) \u2192  \u2502  \u2502  sync(creds) \u2192  \u2502     \u2502\n\u2502  \u2502 normalized[]    \u2502  \u2502  normalized[]   \u2502  \u2502  normalized[]   \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502  [Each adapter: fetch vendor data \u2192 transform \u2192 return normalized]  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               Normalized Data Layer (PostgreSQL + Knex)              \u2502\n\u2502  clients \u2502 sites \u2502 contacts \u2502 users \u2502 devices \u2502 agreements \u2502        \u2502\n\u2502  tickets \u2502 controls \u2502 risks \u2502 recommendations                       \u2502\n\u2502  [Relational schema, indexed for scoring queries]                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Insights Engine (Pure Functions)                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Standards Score  \u2502  \u2502   Risk Score     \u2502  \u2502 Experience Score \u2502  \u2502\n\u2502  \u2502  calculateStd()  \u2502  \u2502  calculateRisk() \u2502  \u2502  calculateExp()  \u2502  \u2502\n\u2502  \u2502  \u2192 0-100 + breakdown \u2502  \u2192 0-100 + breakdown \u2502 \u2192 0-100 + breakdown \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502  [Reads normalized data, applies weighted formulas, returns scores] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Output Layer                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502  \u2502  Narrative Generator \u2502      \u2502   PDF Generator      \u2502             \u2502\n\u2502  \u2502  (OpenAI GPT-4)      \u2502  \u2192   \u2502   (PDFKit)           \u2502             \u2502\n\u2502  \u2502  - Trend spotting    \u2502      \u2502   9 sections:        \u2502             \u2502\n\u2502  \u2502  - Executive summary \u2502      \u2502   1. Executive       \u2502             \u2502\n\u2502  \u2502  - Action synthesis  \u2502      \u2502   2. Score Dashboard \u2502             \u2502\n\u2502  \u2502  - QBR prep notes    \u2502      \u2502   3. Top 5 Risks     \u2502             \u2502\n\u2502  \u2502  [Strict prompts,    \u2502      \u2502   4. Top 5 Wins      \u2502             \u2502\n\u2502  \u2502   citation required] \u2502      \u2502   5. Service Story   \u2502             \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502   6. Security Story  \u2502             \u2502\n\u2502                                \u2502   7. Lifecycle       \u2502             \u2502\n\u2502                                \u2502   8. Roadmap         \u2502             \u2502\n\u2502                                \u2502   9. Outcome Plan    \u2502             \u2502\n\u2502                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Job Queue (pg-boss)                             \u2502\n\u2502  qbr_generation_jobs (client_id, status, result_url, created_at)    \u2502\n\u2502  [Async QBR generation, retry logic, human approval workflow]       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## File Structure\n\n```\netop-am-system-1282/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 adapters/\n\u2502   \u2502   \u251c\u2500\u2500 base-adapter.js           # Abstract interface all adapters implement\n\u2502   \u2502   \u251c\u2500\u2500 connectwise-adapter.js    # CW Manage: agreements, tickets, clients, configs\n\u2502   \u2502   \u251c\u2500\u2500 immy-adapter.js           # Immy.Bot: baselines, drift, compliance\n\u2502   \u2502   \u2514\u2500\u2500 m365-adapter.js           # M365: Entra ID, Defender, Secure Score\n\u2502   \u251c\u2500\u2500 engine/\n\u2502   \u2502   \u251c\u2500\u2500 standards-score.js        # Pure function: data \u2192 0-100 + breakdown\n\u2502   \u2502   \u251c\u2500\u2500 risk-score.js             # Pure function: data \u2192 0-100 + breakdown\n\u2502   \u2502   \u2514\u2500\u2500 experience-score.js       # Pure function: data \u2192 0-100 + breakdown\n\u2502   \u251c\u2500\u2500 qbr/\n\u2502   \u2502   \u251c\u2500\u2500 narrative-generator.js    # OpenAI integration with strict prompts\n\u2502   \u2502   \u2514\u2500\u2500 pdf-generator.js          # PDFKit: scores + narratives \u2192 9-section PDF\n\u2502   \u251c\u2500\u2500 routes/\n\u2502   \u2502   \u251c\u2500\u2500 sync.js                   # POST /sync/connectwise, /sync/immy, /sync/m365\n\u2502   \u2502   \u251c\u2500\u2500 clients.js                # GET /clients/:id/scores\n\u2502   \u2502   \u2514\u2500\u2500 qbr.js                    # POST /clients/:id/qbr/generate, GET /clients/:id/qbr/:jobId\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 client.js                 # Knex query helpers for clients table\n\u2502   \u2502   \u251c\u2500\u2500 device.js                 # Knex query helpers for devices table\n\u2502   \u2502   \u251c\u2500\u2500 ticket.js                 # Knex query helpers for tickets table\n\u2502   \u2502   \u2514\u2500\u2500 ...                       # (one per entity type)\n\u2502   \u251c\u2500\u2500 db/\n\u2502   \u2502   \u251c\u2500\u2500 knexfile.js               # Knex configuration\n\u2502   \u2502   \u2514\u2500\u2500 migrations/               # Timestamped migration files\n\u2502   \u2502       \u251c\u2500\u2500 001_create_clients.js\n\u2502   \u2502       \u251c\u2500\u2500 002_create_sites.js\n\u2502   \u2502       \u251c\u2500\u2500 003_create_devices.js\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 middleware/\n\u2502   \u2502   \u251c\u2500\u2500 auth.js                   # API key validation\n\u2502   \u2502   \u251c\u2500\u2500 error-handler.js          # Centralized error responses\n\u2502   \u2502   \u2514\u2500\u2500 validation.js             # Joi schema validation\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 logger.js                 # Winston configuration\n\u2502   \u2502   \u2514\u2500\u2500 cache.js                  # Simple in-memory cache for scores\n\u2502   \u251c\u2500\u2500 config.js                     # Centralized config (reads from .env)\n\u2502   \u251c\u2500\u2500 app.js                        # Express app setup (routes, middleware)\n\u2502   \u251c\u2500\u2500 server.js                     # HTTP server + graceful shutdown\n\u2502   \u2514\u2500\u2500 worker.js                     # pg-boss worker (processes QBR jobs)\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 unit/\n\u2502   \u2502   \u251c\u2500\u2500 engine/                   # Test scoring formulas with known inputs\n\u2502   \u2502   \u2514\u2500\u2500 adapters/                 # Test adapter normalization logic\n\u2502   \u251c\u2500\u2500 integration/\n\u2502   \u2502   \u2514\u2500\u2500 api/                      # Test API endpoints with test DB\n\u2502   \u251c\u2500\u2500 e2e/\n\u2502   \u2502   \u2514\u2500\u2500 qbr-generation.test.js    # Full flow: mock data \u2192 PDF output\n\u2502   \u2514\u2500\u2500 mocks/\n\u2502       \u251c\u2500\u2500 connectwise-response.json # Sample CW Manage API response\n\u2502       \u251c\u2500\u2500 immy-response.json        # Sample Immy.Bot API response\n\u2502       \u2514\u2500\u2500 m365-response.json        # Sample Microsoft Graph response\n\u251c\u2500\u2500 .env.example                      # Template for required env vars\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 package-lock.json\n\u251c\u2500\u2500 Dockerfile                        # Production container image\n\u251c\u2500\u2500 docker-compose.yml                # Local dev: app + PostgreSQL\n\u251c\u2500\u2500 jest.config.js                    # Jest configuration\n\u2514\u2500\u2500 README.md                         # Setup guide, API docs, architecture overview\n```\n\n---\n\n## Component Interfaces\n\n### API Layer \u2192 Adapter Layer\n\n**Method:** Direct function call  \n**Contract:**\n```javascript\n// All adapters must implement this interface\ninterface BaseAdapter {\n  /**\n   * Sync data from vendor platform\n   * @param {Object} credentials - Vendor-specific auth credentials\n   * @returns {Promise<NormalizedData>} - Normalized entities\n   * @throws {AdapterError} - If API call fails or data invalid\n   */\n  async sync(credentials): Promise<{\n    clients: Client[],\n    sites: Site[],\n    contacts: Contact[],\n    users: User[],\n    devices: Device[],\n    agreements: Agreement[],\n    tickets: Ticket[],\n    controls: Control[],\n    risks: Risk[],\n    recommendations: Recommendation[]\n  }>\n}\n```\n\n**Errors:**\n- `AdapterError` (401: auth failure, 429: rate limit, 500: vendor API down)\n\n---\n\n### Adapter Layer \u2192 Database Layer\n\n**Method:** Knex.js transaction (batch insert/upsert)  \n**Contract:**\n```javascript\n// After adapter returns normalized data, persist to DB\nasync function persistNormalizedData(normalizedData, trx) {\n  // Upsert clients (keyed by external vendor ID)\n  await trx('clients').insert(normalizedData.clients).onConflict('external_id').merge();\n  \n  // Cascade inserts for related entities\n  await trx('sites').insert(normalizedData.sites).onConflict('external_id').merge();\n  // ... repeat for all entities\n  \n  return { synced_at: new Date(), record_counts: {...} };\n}\n```\n\n**Errors:**\n- `DatabaseError` (constraint violation, connection timeout)\n\n---\n\n### API Layer \u2192 Insights Engine\n\n**Method:** Direct function call (pure function, no side effects)  \n**Contract:**\n```javascript\n/**\n * Calculate Standards Compliance score\n * @param {number} clientId - Client database ID\n * @returns {Promise<ScoreResult>}\n */\nasync function calculateStandardsScore(clientId): Promise<{\n  score: number,              // 0-100\n  breakdown: {\n    device_coverage: { score: number, weight: number, evidence: {...} },\n    immy_compliance: { score: number, weight: number, evidence: {...} },\n    patch_compliance: { score: number, weight: number, evidence: {...} },\n    edr_health: { score: number, weight: number, evidence: {...} },\n    m365_secure_score: { score: number, weight: number, evidence: {...} }\n  },\n  computed_at: Date\n}>\n```\n\n**Errors:**\n- `InsufficientDataError` (not enough data to calculate score)\n\n---\n\n### Insights Engine \u2192 Narrative Generator\n\n**Method:** Direct function call (passes scores + evidence)  \n**Contract:**\n```javascript\n/**\n * Generate AI narrative from scores and evidence\n * @param {Object} input - Scores + normalized data excerpts\n * @returns {Promise<Narrative>}\n */\nasync function generateNarrative(input): Promise<{\n  trends: string,               // \"Ticket volume decreased 15% QoQ...\"\n  executive_summary: string,    // Plain English, business outcomes\n  recommendations: Array<{\n    title: string,\n    description: string,\n    priority: 'high' | 'medium' | 'low',\n    evidence: string[]          // REQUIRED: ticket IDs, device names, etc.\n  }>,\n  discussion_points: string[]   // Topics for QBR meeting\n}>\n```\n\n**Errors:**\n- `OpenAIError` (API failure, token limit exceeded, content filter triggered)\n- `HallucinationDetectedError` (validation finds claims without evidence)\n\n---\n\n### Narrative Generator \u2192 PDF Generator\n\n**Method:** Direct function call  \n**Contract:**\n```javascript\n/**\n * Generate QBR PDF\n * @param {Object} data - Scores + narratives + client metadata\n * @returns {Promise<Buffer>} - PDF binary\n */\nasync function generateQBR(data): Promise<Buffer>\n```\n\n**Errors:**\n- `PDFGenerationError` (template error, data formatting issue)\n\n---\n\n### API Layer \u2192 Job Queue\n\n**Method:** pg-boss publish/subscribe  \n**Contract:**\n```javascript\n// Publish QBR generation job\nawait boss.publish('qbr-generation', {\n  client_id: 123,\n  requested_by: 'user@msp.com',\n  options: { include_lifecycle: true }\n});\n\n// Worker subscribes and processes\nboss.subscribe('qbr-generation', async (job) => {\n  const { client_id, options } = job.data;\n  \n  // Execute: scores \u2192 narrative \u2192 PDF\n  const result = await generateFullQBR(client_id, options);\n  \n  return result; // pg-boss stores in job result field\n});\n```\n\n**Errors:**\n- Job retry (3 attempts with exponential backoff)\n- Dead letter queue after max retries\n\n---\n\n## Module Specifications\n\n### Module: ConnectWise Adapter\n**Responsibility:** Fetch and normalize ConnectWise Manage data  \n**Key Files:** `src/adapters/connectwise-adapter.js`  \n**Dependencies:** axios (HTTP), date-fns (date parsing)  \n**Public Interface:**\n```javascript\nclass ConnectWiseAdapter extends BaseAdapter {\n  async sync(credentials): Promise<NormalizedData>\n  \n  // Internal methods (not exported)\n  private async fetchClients()\n  private async fetchAgreements()\n  private async fetchTickets()\n  private async fetchConfigurations()\n  private normalizeClient(cwClient): Client\n  private normalizeTicket(cwTicket): Ticket\n}\n```\n\n---\n\n### Module: Immy.Bot Adapter\n**Responsibility:** Fetch and normalize Immy.Bot compliance data  \n**Key Files:** `src/adapters/immy-adapter.js`  \n**Dependencies:** axios  \n**Public Interface:**\n```javascript\nclass ImmyAdapter extends BaseAdapter {\n  async sync(credentials): Promise<NormalizedData>\n  \n  private async fetchComputers()\n  private async fetchBaselines()\n  private async fetchDriftReports()\n  private normalizeDevice(immyComputer): Device\n  private normalizeControl(immyBaseline): Control\n}\n```\n\n---\n\n### Module: M365 Adapter\n**Responsibility:** Fetch and normalize Microsoft 365 security data  \n**Key Files:** `src/adapters/m365-adapter.js`  \n**Dependencies:** @microsoft/microsoft-graph-client  \n**Public Interface:**\n```javascript\nclass M365Adapter extends BaseAdapter {\n  async sync(credentials): Promise<NormalizedData>\n  \n  private async fetchUsers()           // Entra ID users\n  private async fetchSecureScore()     // Microsoft Secure Score\n  private async fetchDefenderAlerts()  // Defender for Endpoint\n  private normalizeUser(graphUser): User\n  private normalizeRisk(defenderAlert): Risk\n  private normalizeControl(secureScoreControl): Control\n}\n```\n\n---\n\n### Module: Standards Score Engine\n**Responsibility:** Calculate Standards Compliance (0-100)  \n**Key Files:** `src/engine/standards-score.js`  \n**Dependencies:** Knex (database queries)  \n**Public Interface:**\n```javascript\nexport async function calculateStandardsScore(clientId): Promise<ScoreResult>\n\n// Weighted components (internal)\n- deviceCoverageScore()      // 20%: % devices under management\n- immyComplianceScore()      // 30%: % devices passing Immy baselines\n- patchComplianceScore()     // 20%: % devices patched (30-day window)\n- edrHealthScore()           // 15%: EDR installed + reporting\n- m365SecureScore()          // 15%: Microsoft Secure Score normalized\n```\n\n---\n\n### Module: Risk Score Engine\n**Responsibility:** Calculate Risk score (0-100, inverse: higher = more risk)  \n**Key Files:** `src/engine/risk-score.js`  \n**Dependencies:** Knex  \n**Public Interface:**\n```javascript\nexport async function calculateRiskScore(clientId): Promise<ScoreResult>\n\n// Weighted components\n- identityRiskScore()        // 30%: MFA coverage, risky users, stale accounts\n- emailRiskScore()           // 25%: Defender for Office alerts, phishing simulations\n- endpointRiskScore()        // 25%: Unpatched devices, missing EDR, high-risk detections\n- businessModifierScore()    // 20%: Industry (healthcare +10 risk), compliance requirements\n```\n\n---\n\n### Module: Experience Score Engine\n**Responsibility:** Calculate Experience score (0-100)  \n**Key Files:** `src/engine/experience-score.js`  \n**Dependencies:** Knex  \n**Public Interface:**\n```javascript\nexport async function calculateExperienceScore(clientId): Promise<ScoreResult>\n\n// Weighted components\n- ticketsPerUserTrend()      // 25%: Trending down = higher score\n- repeatIssueRate()          // 20%: Same issue recurring = lower score\n- slaPerformance()           // 25%: % tickets meeting SLA\n- reopenRate()               // 15%: % tickets reopened\n- afterHoursIncidents()      // 15%: Fewer = higher score\n```\n\n---\n\n### Module: Narrative Generator\n**Responsibility:** Generate AI narrative with citations  \n**Key Files:** `src/qbr/narrative-generator.js`  \n**Dependencies:** openai (OpenAI SDK)  \n**Public Interface:**\n```javascript\nexport async function generateNarrative(input: {\n  client: Client,\n  scores: { standards: ScoreResult, risk: ScoreResult, experience: ScoreResult },\n  recent_tickets: Ticket[],\n  top_risks: Risk[],\n  lifecycle_items: Device[]\n}): Promise<Narrative>\n\n// Internal: four OpenAI jobs\n- spotTrends()               // Analyze quarter-over-quarter movement\n- writeExecutiveSummary()    // Plain English business outcomes\n- synthesizeActions()        // Recommendations with effort/cost\n- prepareDiscussionPoints()  // Topics for live QBR meeting\n```\n\n**Prompt Template (Example for Executive Summary):**\n```\nYou are an Account Manager writing a quarterly business review for {client.name}.\n\nSTRICT RULES:\n1. Only use facts from the PROVIDED DATA below. Never invent ticket IDs, device names, or metrics.\n2. Every claim must cite evidence: [Ticket #12345], [Device: WS-ACCT-01], [Policy: MFA Enforcement].\n3. Write in plain English for a non-technical executive audience.\n4. Focus on business outcomes (user productivity, risk reduction, cost avoidance), not technical details.\n\nPROVIDED DATA:\n{JSON.stringify(input, null, 2)}\n\nWrite a 3-paragraph executive summary:\n1. Overall health this quarter (reference the three scores)\n2. Biggest win (with evidence)\n3. Top priority for next quarter (with business justification)\n```\n\n---\n\n### Module: PDF Generator\n**Responsibility:** Render 9-section QBR PDF  \n**Key Files:** `src/qbr/pdf-generator.js`  \n**Dependencies:** pdfkit, pdfkit-table (tables)  \n**Public Interface:**\n```javascript\nexport async function generateQBR(data: {\n  client: Client,\n  scores: ScoreResults,\n  narrative: Narrative,\n  risks: Risk[],\n  wins: string[],\n  lifecycle: Device[],\n  roadmap: Recommendation[]\n}): Promise<Buffer>\n\n// Internal: one function per section\n- renderExecutiveSummary()\n- renderScoreDashboard()      // Gauge charts for 3 scores\n- renderTopRisks()            // Table with business impact\n- renderTopWins()             // Bulleted list\n- renderServiceStory()        // Ticket trend charts\n- renderSecurityStory()       // Identity/Email/Endpoint breakdown\n- renderLifecycleForecast()   // Aging device table + 12-36mo budget\n- renderRoadmap()             // Now/Next/Later columns\n- renderOutcomePlan()         // Action items with owners + dates\n```\n\n---\n\n## Data Models\n\n### PostgreSQL Schema (Normalized)\n\n```sql\n-- Core entities\nCREATE TABLE clients (\n  id SERIAL PRIMARY KEY,\n  external_id VARCHAR(255) UNIQUE NOT NULL, -- Vendor-specific ID\n  source VARCHAR(50) NOT NULL,              -- 'connectwise', 'immy', 'm365'\n  name VARCHAR(255) NOT NULL,\n  segment CHAR(1) CHECK (segment IN ('A', 'B', 'C', 'D')),\n  mrr DECIMAL(10,2),\n  agreement_start DATE,\n  agreement_end DATE,\n  created_at TIMESTAMP DEFAULT NOW(),\n  updated_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_clients_external ON clients(external_id, source);\n\nCREATE TABLE sites (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  name VARCHAR(255),\n  address TEXT,\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_sites_client ON sites(client_id);\n\nCREATE TABLE contacts (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  role VARCHAR(100),                        -- 'CEO', 'CFO', 'IT Manager', etc.\n  email VARCHAR(255),\n  phone VARCHAR(50),\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_contacts_client ON contacts(client_id);\n\nCREATE TABLE users (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  email VARCHAR(255),\n  upn VARCHAR(255),                         -- User Principal Name (M365)\n  mfa_enabled BOOLEAN DEFAULT FALSE,\n  risk_level VARCHAR(20),                   -- 'none', 'low', 'medium', 'high'\n  last_sign_in TIMESTAMP,\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_users_client ON users(client_id);\nCREATE INDEX idx_users_risk ON users(risk_level);\n\nCREATE TABLE devices (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  site_id INTEGER REFERENCES sites(id) ON DELETE SET NULL,\n  external_id VARCHAR(255),\n  name VARCHAR(255),\n  type VARCHAR(50),                         -- 'endpoint', 'server', 'network'\n  os VARCHAR(100),\n  managed BOOLEAN DEFAULT FALSE,\n  health_status VARCHAR(20),                -- 'healthy', 'warning', 'critical'\n  last_seen TIMESTAMP,\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_devices_client ON devices(client_id);\nCREATE INDEX idx_devices_health ON devices(health_status);\n\nCREATE TABLE agreements (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  mrr DECIMAL(10,2),\n  effective_rate DECIMAL(10,2),\n  term_months INTEGER,\n  start_date DATE,\n  end_date DATE,\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE tickets (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  category VARCHAR(100),\n  priority VARCHAR(20),\n  status VARCHAR(50),\n  hours_spent DECIMAL(5,2),\n  sla_met BOOLEAN,\n  reopen_count INTEGER DEFAULT 0,\n  csat_score INTEGER,                       -- 1-5 or NULL\n  created_date TIMESTAMP,\n  closed_date TIMESTAMP,\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_tickets_client ON tickets(client_id);\nCREATE INDEX idx_tickets_created ON tickets(created_date);\n\nCREATE TABLE controls (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  control_type VARCHAR(100),                -- 'immy_baseline', 'm365_secure_score', etc.\n  status VARCHAR(20),                       -- 'pass', 'fail', 'unknown'\n  evidence JSONB,                           -- Vendor-specific proof\n  last_checked TIMESTAMP,\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_controls_client ON controls(client_id);\nCREATE INDEX idx_controls_status ON controls(status);\n\nCREATE TABLE risks (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  external_id VARCHAR(255),\n  risk_type VARCHAR(100),                   -- 'identity', 'email', 'endpoint', 'business'\n  title VARCHAR(255),\n  description TEXT,\n  likelihood VARCHAR(20),                   -- 'low', 'medium', 'high'\n  impact VARCHAR(20),                       -- 'low', 'medium', 'high'\n  status VARCHAR(20),                       -- 'open', 'mitigated', 'accepted'\n  detected_at TIMESTAMP,\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_risks_client ON risks(client_id);\nCREATE INDEX idx_risks_status ON risks(status);\n\nCREATE TABLE recommendations (\n  id SERIAL PRIMARY KEY,\n  client_id INTEGER REFERENCES clients(id) ON DELETE CASCADE,\n  title VARCHAR(255),\n  description TEXT,\n  effort VARCHAR(20),                       -- 'low', 'medium', 'high'\n  cost_range VARCHAR(50),                   -- '$1-5K', '$5-10K', etc.\n  priority VARCHAR(20),                     -- 'high', 'medium', 'low'\n  quarter VARCHAR(10),                      -- 'Q1 2026', 'Q2 2026', etc.\n  status VARCHAR(20) DEFAULT 'pending',     -- 'pending', 'approved', 'completed'\n  created_at TIMESTAMP DEFAULT NOW()\n);\nCREATE INDEX idx_recommendations_client ON recommendations(client_id);\n\n-- Job queue (pg-boss auto-creates tables, but documenting schema)\n-- pg-boss manages: job, archive, version, schedule tables\n```\n\n---\n\n## Implementation Steps\n\n1. **Project Setup**\n   - Initialize Node.js project (`npm init`)\n   - Install dependencies (Express, Knex, PostgreSQL driver, PDFKit, OpenAI, pg-boss)\n   - Create `.env.example` with required environment variables\n   - Set up ESLint + Prettier\n\n2. **Database Layer**\n   - Configure Knex (`knexfile.js`)\n   - Write migrations for all 10 entity tables\n   - Create model files (query helpers for each table)\n   - Seed test database with mock data\n\n3. **Adapter Layer (Critical Path)**\n   - Implement `BaseAdapter` abstract class\n   - Build ConnectWise adapter (clients, agreements, tickets, devices)\n   - Build Immy adapter (devices, controls)\n   - Build M365 adapter (users, risks, controls)\n   - Write unit tests with mocked API responses\n\n4. **API Routes (Sync)**\n   - `POST /sync/connectwise` \u2192 calls adapter \u2192 persists to DB\n   - `POST /sync/immy` \u2192 calls adapter \u2192 persists to DB\n   - `POST /sync/m365` \u2192 calls adapter \u2192 persists to DB\n   - Add API key auth middleware\n   - Add request validation (Joi schemas)\n\n5. **Scoring Engine**\n   - Implement `calculateStandardsScore()` with weighted components\n   - Implement `calculateRiskScore()` with weighted components\n   - Implement `calculateExperienceScore()` with weighted components\n   - Write unit tests with known input/output pairs\n   - Create `GET /clients/:id/scores` endpoint\n\n6. **Narrative Generator**\n   - Create OpenAI prompt templates (4 jobs: trends, summary, actions, prep)\n   - Implement citation validation (regex check for ticket IDs, device names)\n   - Add retry logic for OpenAI API failures\n   - Write tests with mocked OpenAI responses\n\n7. **PDF Generator**\n   - Scaffold 9-section PDF structure with PDFKit\n   - Implement gauge charts for score dashboard\n   - Implement tables for risks, lifecycle, roadmap\n   - Test with various data volumes (edge case: client with 500 devices)\n\n8. **Job Queue (Async QBR Generation)**\n   - Set up pg-boss\n   - Create `qbr-generation` job handler (scores \u2192 narrative \u2192 PDF)\n   - Implement `POST /clients/:id/qbr/generate` (publishes job, returns job ID)\n   - Implement `GET /clients/:id/qbr/:jobId` (checks job status, returns PDF URL)\n   - Add retry logic (3 attempts)\n\n9. **End-to-End Testing**\n   - Seed test DB with mock data from all 3 adapters\n   - Trigger full QBR generation flow\n   - Validate PDF output contains all 9 sections with real data\n   - Verify OpenAI citations reference actual ticket IDs\n\n10. **Deployment Prep**\n    - Create Dockerfile (multi-stage build)\n    - Create `docker-compose.yml` (app + PostgreSQL)\n    - Document environment variables in README\n    - Add health check endpoint (`GET /health`)\n\n---\n\n## Testing Strategy\n\n### Unit Tests\n- **Adapters**: Test normalization logic with mocked vendor API responses\n  - Input: `connectwise-response.json` \u2192 Output: `Client[]` with correct fields\n  - Validate error handling (401, 429, 500 from vendor API)\n- **Scoring Engine**: Test formulas with known inputs\n  - Example: 80% devices managed, 90% Immy compliance \u2192 Standards score = 85\n  - Test edge cases: zero devices, missing data\n- **Narrative Generator**: Test prompt construction and citation extraction\n  - Mock OpenAI response \u2192 validate citations match provided ticket IDs\n  - Test hallucination detection (claim without evidence \u2192 error)\n\n### Integration Tests\n- **API Endpoints**: Test routes with test database\n  - `POST /sync/connectwise` \u2192 verify DB tables populated\n  - `GET /clients/:id/scores` \u2192 verify scores calculated from DB data\n  - `POST /clients/:id/qbr/generate` \u2192 verify job created in queue\n- **Database**: Test migrations run cleanly (up/down)\n- **Job Queue**: Test pg-boss job processing with mocked QBR generation\n\n### End-to-End Tests\n- **Full QBR Flow**:\n  1. Seed DB with mock data from `tests/mocks/`\n  2. Trigger `POST /clients/1/qbr/generate`\n  3. Wait for job completion\n  4. Download PDF\n  5. Validate PDF contains:\n     - All 9 sections\n     - Real data (ticket IDs, device names from mocked data)\n     - OpenAI narratives with citations\n     - Charts render correctly\n\n### Performance Tests (Future)\n- Sync 1000 tickets from ConnectWise \u2192 measure DB insert time\n- Generate QBR for client with 500 devices \u2192 measure total time (target: <60s)\n\n---\n\n## Success Criteria\n\nPhase 1 is **DONE** when:\n\n1. \u2705 **Adapter Pattern Proven**\n   - All 3 adapters sync mock data \u2192 normalized DB tables populated\n   - Adding a 4th adapter requires zero changes to engine/qbr modules\n\n2. \u2705 **Scores Calculate Correctly**\n   - Standards, Risk, Experience scores return 0-100 values\n   - Breakdown explains weighted components with evidence\n   - Edge case: insufficient data \u2192 returns error, not invalid score\n\n3. \u2705 **OpenAI Generates Trustworthy Narratives**\n   - Every recommendation cites evidence (ticket IDs, device names)\n   - Hallucination validation catches claims without evidence\n   - Narratives are plain English suitable for non-technical executives\n\n4. \u2705 **PDF Contains All 9 Sections with Real Data**\n   - Executive Summary (narratives)\n   - Score Dashboard (3 gauges with movement explanation)\n   - Top 5 Risks (table with business impact)\n   - Top 5 Wins (list)\n   - Service Experience Story (ticket trends)\n   - Security Posture Story (identity/email/endpoint)\n   - Lifecycle Forecast (aging devices + budget)\n   - Roadmap (Now/Next/Later)\n   - Outcome Plan (actions with owners/dates)\n\n5. \u2705 **End-to-End Test Passes**\n   - Mock data \u2192 sync \u2192 score \u2192 QBR PDF generation succeeds\n   - PDF visually reviewed for correctness\n   - No errors in logs\n\n6. \u2705 **Documentation Complete**\n   - README explains setup, API usage, environment variables\n   - Architecture decision log documented\n   - Code comments explain scoring formulas and weights\n\n---\n\n## Acceptance Criteria (Gherkin)\n\n### Feature: ConnectWise Adapter Sync\n\nSyncs clients, agreements, tickets, and devices from ConnectWise Manage and normalizes them into the database.\n\n**Background:**\n```gherkin\nGiven the database is empty\nAnd I have valid ConnectWise Manage credentials in environment variables\nAnd ConnectWise Manage contains:\n  | Entity      | Count |\n  | Clients     | 5     |\n  | Agreements  | 8     |\n  | Tickets     | 150   |\n  | Devices     | 47    |\n```\n\n**Scenario: Successfully sync all ConnectWise data**\n```gherkin\nWhen I POST to \"/sync/connectwise\" with valid credentials\nThen the response status should be 200\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"synced_at\": \"<timestamp>\",\n    \"record_counts\": {\n      \"clients\": 5,\n      \"agreements\": 8,\n      \"tickets\": 150,\n      \"devices\": 47\n    }\n  }\n  \"\"\"\nAnd the \"clients\" table should contain 5 records\nAnd the \"agreements\" table should contain 8 records\nAnd the \"tickets\" table should contain 150 records\nAnd the \"devices\" table should contain 47 records\nAnd each client record should have a valid \"external_id\" from ConnectWise\nAnd each client record should have \"source\" = \"connectwise\"\n```\n\n**Scenario: Handle ConnectWise API authentication failure**\n```gherkin\nGiven I have invalid ConnectWise credentials\nWhen I POST to \"/sync/connectwise\" with invalid credentials\nThen the response status should be 401\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"error\": \"ConnectWise authentication failed\",\n    \"details\": \"Invalid API credentials\"\n  }\n  \"\"\"\nAnd no records should be inserted into the database\n```\n\n**Scenario: Handle ConnectWise API rate limit**\n```gherkin\nGiven ConnectWise API returns 429 status (rate limit exceeded)\nWhen I POST to \"/sync/connectwise\"\nThen the adapter should retry with exponential backoff\nAnd after 3 retries, the response status should be 429\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"error\": \"ConnectWise rate limit exceeded\",\n    \"retry_after\": 60\n  }\n  \"\"\"\n```\n\n---\n\n### Feature: Immy.Bot Adapter Sync\n\nSyncs device compliance data from Immy.Bot and normalizes it into devices and controls tables.\n\n**Background:**\n```gherkin\nGiven the database contains 5 clients from ConnectWise sync\nAnd I have valid Immy.Bot credentials\nAnd Immy.Bot contains:\n  | Entity          | Count |\n  | Computers       | 47    |\n  | Baselines       | 3     |\n  | Drift Reports   | 12    |\n```\n\n**Scenario: Successfully sync Immy.Bot compliance data**\n```gherkin\nWhen I POST to \"/sync/immy\" with valid credentials\nThen the response status should be 200\nAnd the \"devices\" table should be updated with 47 records (upsert by external_id)\nAnd the \"controls\" table should contain 12 records with control_type = \"immy_baseline\"\nAnd each control record should have:\n  | Field         | Value                      |\n  | status        | \"pass\", \"fail\", or \"unknown\" |\n  | evidence      | Valid JSON object          |\n  | last_checked  | Timestamp within last 24h  |\n```\n\n**Scenario: Immy.Bot device maps to existing ConnectWise device**\n```gherkin\nGiven a ConnectWise device exists with external_id = \"CW-DEVICE-123\"\nAnd an Immy.Bot computer has the same serial number mapping to \"CW-DEVICE-123\"\nWhen I POST to \"/sync/immy\"\nThen the device record should be updated (not duplicated)\nAnd the device should have data from both ConnectWise and Immy.Bot sources\nAnd the \"managed\" field should be set to true\n```\n\n---\n\n### Feature: Microsoft 365 Adapter Sync\n\nSyncs users, security controls, and risks from Microsoft Graph API.\n\n**Background:**\n```gherkin\nGiven the database contains 5 clients\nAnd I have valid Microsoft 365 credentials (tenant ID, client ID, client secret)\nAnd Microsoft Graph API contains:\n  | Entity               | Count |\n  | Entra ID Users       | 73    |\n  | Secure Score Controls| 28    |\n  | Defender Alerts      | 5     |\n```\n\n**Scenario: Successfully sync Microsoft 365 data**\n```gherkin\nWhen I POST to \"/sync/m365\" with valid credentials\nThen the response status should be 200\nAnd the \"users\" table should contain 73 records\nAnd the \"controls\" table should contain 28 records with control_type = \"m365_secure_score\"\nAnd the \"risks\" table should contain 5 records with risk_type = \"endpoint\"\nAnd each user record should have:\n  | Field        | Constraint                     |\n  | email        | Valid email format             |\n  | upn          | Matches Microsoft UPN format   |\n  | mfa_enabled  | Boolean (true/false)           |\n  | risk_level   | \"none\", \"low\", \"medium\", or \"high\" |\n```\n\n**Scenario: Microsoft Graph API permission error**\n```gherkin\nGiven the Microsoft 365 credentials lack required Graph API scopes\nWhen I POST to \"/sync/m365\"\nThen the response status should be 403\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"error\": \"Microsoft Graph permission denied\",\n    \"required_scopes\": [\"User.Read.All\", \"SecurityEvents.Read.All\", \"SecurityActions.Read.All\"]\n  }\n  \"\"\"\n```\n\n---\n\n### Feature: Calculate Standards Compliance Score\n\nCalculates a 0-100 score based on device coverage, Immy compliance, patch status, EDR health, and M365 Secure Score.\n\n**Background:**\n```gherkin\nGiven client ID 1 has the following data:\n  | Metric                  | Value |\n  | Total devices           | 50    |\n  | Managed devices         | 45    |\n  | Devices passing Immy    | 40    |\n  | Devices patched (30d)   | 42    |\n  | Devices with EDR        | 48    |\n  | M365 Secure Score       | 72%   |\n```\n\n**Scenario: Calculate Standards score with all data present**\n```gherkin\nWhen I GET \"/clients/1/scores\"\nThen the response status should be 200\nAnd the response body should contain a \"standards\" object with:\n  | Field | Value Range |\n  | score | 80-85       |\nAnd the \"standards.breakdown\" should include:\n  | Component         | Weight | Score Range |\n  | device_coverage   | 20%    | 90-100      |\n  | immy_compliance   | 30%    | 80-90       |\n  | patch_compliance  | 20%    | 84-90       |\n  | edr_health        | 15%    | 96-100      |\n  | m365_secure_score | 15%    | 72-72       |\nAnd each breakdown component should have an \"evidence\" object with source data\n```\n\n**Scenario: Insufficient data for Standards score**\n```gherkin\nGiven client ID 2 has zero devices in the database\nWhen I GET \"/clients/2/scores\"\nThen the response status should be 200\nAnd the \"standards\" object should have:\n  \"\"\"\n  {\n    \"score\": null,\n    \"error\": \"Insufficient data: no devices found\",\n    \"breakdown\": {}\n  }\n  \"\"\"\n```\n\n---\n\n### Feature: Calculate Risk Score\n\nCalculates a 0-100 score (inverse: higher = more risk) based on identity, email, endpoint, and business risks.\n\n**Background:**\n```gherkin\nGiven client ID 1 has the following data:\n  | Metric                     | Value |\n  | Total users                | 50    |\n  | Users with MFA enabled     | 45    |\n  | Users with high risk level | 2     |\n  | Defender email alerts      | 3     |\n  | Devices missing EDR        | 2     |\n  | Open critical risks        | 1     |\n```\n\n**Scenario: Calculate Risk score with good security posture**\n```gherkin\nWhen I GET \"/clients/1/scores\"\nThen the response status should be 200\nAnd the \"risk\" object should have a score between 10-20 (low risk)\nAnd the \"risk.breakdown\" should include:\n  | Component           | Weight | Risk Level |\n  | identity_risk       | 30%    | Low        |\n  | email_risk          | 25%    | Low        |\n  | endpoint_risk       | 25%    | Low        |\n  | business_modifier   | 20%    | Medium     |\nAnd the \"risk.breakdown.identity_risk.evidence\" should cite:\n  - \"45/50 users (90%) have MFA enabled\"\n  - \"2 users flagged as high risk by Entra ID\"\n```\n\n**Scenario: Calculate Risk score with poor MFA coverage**\n```gherkin\nGiven client ID 3 has:\n  | Metric                 | Value |\n  | Total users            | 100   |\n  | Users with MFA enabled | 30    |\nWhen I GET \"/clients/3/scores\"\nThen the \"risk.breakdown.identity_risk\" should have a score >= 50 (high risk)\nAnd the evidence should cite \"30/100 users (30%) have MFA enabled\"\n```\n\n---\n\n### Feature: Calculate Experience Score\n\nCalculates a 0-100 score based on ticket volume, repeat issues, SLA performance, reopen rate, and after-hours incidents.\n\n**Background:**\n```gherkin\nGiven client ID 1 has the following ticket data for Q4 2025:\n  | Metric                     | Value |\n  | Total tickets              | 120   |\n  | Total users                | 50    |\n  | Tickets meeting SLA        | 108   |\n  | Tickets reopened           | 5     |\n  | Repeat issues (same cat.)  | 8     |\n  | After-hours tickets        | 12    |\nAnd client ID 1 had 150 tickets in Q3 2025\n```\n\n**Scenario: Calculate Experience score with improving trend**\n```gherkin\nWhen I GET \"/clients/1/scores\"\nThen the response status should be 200\nAnd the \"experience\" object should have a score between 75-85\nAnd the \"experience.breakdown.tickets_per_user_trend\" should cite:\n  - \"Q3 2025: 3.0 tickets/user\"\n  - \"Q4 2025: 2.4 tickets/user\"\n  - \"Improvement: -20%\"\nAnd the \"experience.breakdown.sla_performance\" should have a score >= 85\nAnd the evidence should cite \"108/120 tickets (90%) met SLA\"\n```\n\n**Scenario: High reopen rate lowers Experience score**\n```gherkin\nGiven client ID 2 has 40 tickets with 15 reopens (37.5% reopen rate)\nWhen I GET \"/clients/2/scores\"\nThen the \"experience.breakdown.reopen_rate\" should have a score <= 30\nAnd the evidence should cite \"15/40 tickets (37.5%) were reopened\"\n```\n\n---\n\n### Feature: Generate QBR Narrative with OpenAI\n\nUses OpenAI GPT-4 to generate executive summary, trends, recommendations, and discussion points with mandatory citations.\n\n**Background:**\n```gherkin\nGiven client ID 1 has scores:\n  | Score      | Value |\n  | Standards  | 83    |\n  | Risk       | 15    |\n  | Experience | 78    |\nAnd the database contains:\n  - 120 tickets for Q4 2025\n  - 3 open risks with risk_type = \"identity\"\n  - 5 devices flagged for lifecycle replacement\n```\n\n**Scenario: Generate narrative with valid citations**\n```gherkin\nWhen I POST to \"/clients/1/qbr/generate\"\nThen a job should be created in the queue\nAnd when the job completes, the narrative should include:\n  - \"executive_summary\" (3 paragraphs, plain English, business outcomes)\n  - \"trends\" (quarter-over-quarter analysis)\n  - \"recommendations\" (array of 3-7 items)\n  - \"discussion_points\" (array of 3-5 topics)\nAnd every recommendation should have an \"evidence\" array with at least 1 citation\nAnd citations should match one of these formats:\n  - \"[Ticket #12345]\"\n  - \"[Device: WS-ACCT-01]\"\n  - \"[Policy: MFA Enforcement]\"\n  - \"[Risk: Unpatched Exchange Server]\"\nAnd no recommendation should contain invented data (e.g., ticket IDs not in database)\n```\n\n**Scenario: Detect hallucination in OpenAI output**\n```gherkin\nGiven the OpenAI response contains a recommendation:\n  \"\"\"\n  {\n    \"title\": \"Replace aging firewall\",\n    \"description\": \"Device FW-MAIN-99 is end-of-life\",\n    \"evidence\": [\"[Device: FW-MAIN-99]\"]\n  }\n  \"\"\"\nBut the database does not contain a device with name \"FW-MAIN-99\"\nWhen the narrative validation runs\nThen the validation should fail with error:\n  \"\"\"\n  Hallucination detected: Device \"FW-MAIN-99\" cited in evidence but not found in database\n  \"\"\"\nAnd the job status should be \"failed\"\nAnd a human review flag should be set\n```\n\n**Scenario: OpenAI API timeout**\n```gherkin\nGiven the OpenAI API takes longer than 60 seconds to respond\nWhen the narrative generation job runs\nThen the job should retry up to 3 times with exponential backoff\nAnd if all retries fail, the job status should be \"failed\"\nAnd the error message should be \"OpenAI API timeout after 3 retries\"\n```\n\n---\n\n### Feature: Generate QBR PDF\n\nProduces a 9-section PDF with scores, narratives, risks, wins, lifecycle forecast, and roadmap.\n\n**Background:**\n```gherkin\nGiven client ID 1 has:\n  - Scores: Standards 83, Risk 15, Experience 78\n  - Narrative generated by OpenAI (all sections present)\n  - 5 open risks in database\n  - 8 recommendations for next quarter\n  - 12 devices flagged for lifecycle replacement\n```\n\n**Scenario: Generate complete QBR PDF**\n```gherkin\nWhen the QBR generation job for client ID 1 completes\nThen a PDF file should be created\nAnd the PDF should contain exactly 9 sections in this order:\n  1. Executive Summary\n  2. Score Dashboard (3 gauge charts)\n  3. Top 5 Risks\n  4. Top 5 Wins\n  5. Service Experience Story\n  6. Security Posture Story\n  7. Lifecycle Forecast\n  8. Roadmap (Now/Next/Later)\n  9. Outcome Plan\nAnd the \"Score Dashboard\" section should display:\n  - Standards score: 83 (with visual gauge)\n  - Risk score: 15 (with visual gauge)\n  - Experience score: 78 (with visual gauge)\n  - Movement explanation (e.g., \"Standards +5 vs. Q3\")\nAnd the \"Top 5 Risks\" section should display a table with columns:\n  - Risk Title\n  - Business Impact\n  - Status\n  - Mitigation Action\nAnd the table should contain exactly 5 rows (top risks by likelihood \u00d7 impact)\nAnd the \"Lifecycle Forecast\" section should display:\n  - Table of devices aged 4+ years\n  - 12-month refresh budget estimate\n  - 24-month refresh budget estimate\n  - 36-month refresh budget estimate\n```\n\n**Scenario: PDF renders correctly with large dataset**\n```gherkin\nGiven client ID 2 has 500 devices and 1000 tickets\nWhen the QBR PDF is generated\nThen the PDF should be created without errors\nAnd the file size should be less than 10 MB\nAnd all sections should render completely (no truncation)\n```\n\n**Scenario: Missing data in PDF**\n```gherkin\nGiven client ID 3 has no recommendations in the database\nWhen the QBR PDF is generated\nThen the \"Roadmap\" section should display:\n  \"\"\"\n  No recommendations available for this quarter.\n  \"\"\"\nAnd the PDF should still contain all 9 sections\n```\n\n---\n\n### Feature: Async QBR Generation with Job Queue\n\nQBR generation is asynchronous using pg-boss. Client receives a job ID and can poll for status.\n\n**Background:**\n```gherkin\nGiven client ID 1 exists with all required data\nAnd the pg-boss worker is running\n```\n\n**Scenario: Submit QBR generation job**\n```gherkin\nWhen I POST to \"/clients/1/qbr/generate\"\nThen the response status should be 202 (Accepted)\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"job_id\": \"<uuid>\",\n    \"status\": \"queued\",\n    \"message\": \"QBR generation started. Poll /clients/1/qbr/<job_id> for status.\"\n  }\n  \"\"\"\n```\n\n**Scenario: Poll job status (in progress)**\n```gherkin\nGiven a QBR generation job exists with ID \"abc-123\"\nAnd the job status is \"active\"\nWhen I GET \"/clients/1/qbr/abc-123\"\nThen the response status should be 200\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"job_id\": \"abc-123\",\n    \"status\": \"active\",\n    \"progress\": \"Generating narrative (step 2/4)\"\n  }\n  \"\"\"\n```\n\n**Scenario: Poll job status (completed)**\n```gherkin\nGiven a QBR generation job exists with ID \"abc-123\"\nAnd the job status is \"completed\"\nAnd the PDF file is stored at \"/outputs/qbr-client-1-abc-123.pdf\"\nWhen I GET \"/clients/1/qbr/abc-123\"\nThen the response status should be 200\nAnd the response body should contain:\n  \"\"\"\n  {\n    \"job_id\": \"abc-123\",\n    \"status\": \"completed\",\n    \"pdf_url\": \"/outputs/qbr-client-1-abc-123.pdf\",\n    \"completed_at\": \"<timestamp>\"\n  }\n  \"\"\"\n```\n\n**Scenario: Job fails and retries**\n```gherkin\nGiven the OpenAI API is temporarily down\nWhen a QBR generation job is processed\nThen the job should fail with status \"retry\"\nAnd the job should be retried after 30 seconds (exponential backoff)\nAnd after 3 failed attempts, the job status should be \"failed\"\nAnd the error message should be logged\n```\n\n---\n\n### Feature: Client Segmentation for QBR Cadence\n\nClients are segmented (A/B/C/D) to determine QBR frequency. Segment affects cadence, not content.\n\n**Background:**\n```gherkin\nGiven the database contains clients with segments:\n  | Client ID | Segment | Last QBR Date |\n  | 1         | A       | 2025-10-01    |\n  | 2         | B       | 2025-10-01    |\n  | 3         | C       | 2025-07-01    |\n  | 4         | D       | 2024-10-01    |\nAnd today is 2026-01-15\n```\n\n**Scenario: Segment A/B clients are due for quarterly QBR**\n```gherkin\nWhen I GET \"/clients/due-for-qbr\"\nThen the response should include clients 1 and 2\nAnd the response should explain:\n  - Client 1 (Segment A): Last QBR 2025-10-01, due quarterly, next due 2026-01-01\n  - Client 2 (Segment B): Last QBR 2025-10-01, due quarterly, next due 2026-01-01\n```\n\n**Scenario: Segment C clients are due for semiannual QBR**\n```gherkin\nWhen I GET \"/clients/due-for-qbr\"\nThen the response should include client 3\nAnd the response should explain:\n  - Client 3 (Segment C): Last QBR 2025-07-01, due semiannually, next due 2026-01-01\n```\n\n**Scenario: Segment D clients are due for annual QBR**\n```gherkin\nWhen I GET \"/clients/due-for-qbr\"\nThen the response should include client 4\nAnd the response should explain:\n  - Client 4 (Segment D): Last QBR 2024-10-01, due annually, next due 2025-10-01 (overdue)\n```\n\n**Scenario: All segments receive same QBR quality**\n```gherkin\nGiven I generate QBRs for client 1 (Segment A) and client 4 (Segment D)\nThen both PDFs should contain all 9 sections\nAnd both PDFs should have OpenAI-generated narratives with citations\nAnd both PDFs should have the same level of detail and formatting\n```", "is_error": false, "exit_code": 0}
